{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e35689-ea06-4f03-9a61-9e91717a1aee",
   "metadata": {},
   "source": [
    "### Project Introduction\n",
    "- **Goal**: The goal of this project is to create a crossword helper that provides the user additional hints/clues to make solving crosswords more enjoyable. For now, the crossword helper assumes that you have access to the correct answers, but the end product would not require this. Moreover, a separate computer vision piece is being developed so a user can just take a picture of the entire crossword and request help where needed. For now, the crossword will operate on a clue/answer pair as being the input. As far as hint generation goes, the project is heading in a few different directions with varying levels of complexity, which include but are not limited to:\n",
    "  1. Provide synonyms/related words/antonyms to the answer --> use embeddings/thesaurus\n",
    "  2. Provide answer classification so the user know what *kind* of word they should be thinking of --> classification problem, probably exists\n",
    "  3. Provide clue classification so the user knows what *kind* of hint they are looking at --> classification problem\n",
    "  4. Provide new additional hints so the user can look at an answer from a different perspective --> train a transformer?\n",
    "- Data: The data used in this project consists of NYT crossword data from 1993-2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826e67-d3dc-456b-b6e9-2c489d2ee754",
   "metadata": {},
   "source": [
    "### Initial Data Inspection, Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0874b76d-2fe4-48e1-9c51-6a4cb3b9c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result['encoding'])  # Displays the detected encoding\n",
    "\n",
    "df = pd.read_csv('nytcrosswords.csv', encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542b4d5a-bcb7-4a00-b4b1-eeb6b9170eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23420 entries, 0 to 23419\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Date             23420 non-null  datetime64[ns]\n",
      " 1   Word             23420 non-null  object        \n",
      " 2   Clue             23420 non-null  object        \n",
      " 3   Character Count  23420 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 732.0+ KB\n"
     ]
    }
   ],
   "source": [
    "### Minimal Cleaning for Deep Learning \n",
    "#drop any null rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#simple cleaning - get rid of excess whitespace, let BERT handle the rest!\n",
    "df['Clue'] = df['Clue'].str.strip()\n",
    "df['Word'] = df['Word'].str.strip()\n",
    "df['Date'] = pd.to_datetime(df['Date'], format = '%m/%d/%Y')\n",
    "\n",
    "#Add character count column that shows the length of each answer\n",
    "df[\"Character Count\"] = df[\"Word\"].apply(len)\n",
    "\n",
    "#filter to 2021 for smaller dataset\n",
    "df = df[df['Date'].dt.year == 2021]\n",
    "\n",
    "#shuffle to reduce bias\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#let's add a column that tells you how many characters \n",
    "df.info()\n",
    "df.to_csv('deep_learning_nytcrosswords2021.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f980c81-d86a-4cdc-b1b6-dd9607581fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>Character Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-25</td>\n",
       "      <td>STYE</td>\n",
       "      <td>Eyelid affliction</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Word               Clue  Character Count\n",
       "0 2021-10-25  STYE  Eyelid affliction                4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d40759-d85b-4186-867f-3095b5238cb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Advanced Data Loading - Batch Processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42803b-b78c-419e-b437-8f8ea9e66dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Smaller Data Solution - Pandas and Pytorch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Detect file encoding\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "# Define batch size and chunk size for efficient loading\n",
    "batch_size = 16\n",
    "chunk_size = 10000  # Adjust based on memory and performance\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define PyTorch Dataset class\n",
    "class CrosswordDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.clues = df[\"Clue\"].tolist()\n",
    "        self.answers = df[\"Word\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clue = self.clues[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        # Tokenize clue\n",
    "        encoding = tokenizer(clue, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": answer\n",
    "        }\n",
    "\n",
    "# Load CSV in chunks and process data in batches\n",
    "chunks = pd.read_csv('nytcrosswords.csv', encoding=encoding, chunksize=chunk_size)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Clean and filter data\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunk['Clue'] = chunk['Clue'].str.strip()\n",
    "    chunk['Word'] = chunk['Word'].str.strip()\n",
    "    chunk['Date'] = pd.to_datetime(chunk['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "    chunk = chunk[chunk['Date'].dt.year == 2021]\n",
    "\n",
    "    # Convert to PyTorch dataset and DataLoader\n",
    "    dataset = CrosswordDataset(chunk)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop (simplified example)\n",
    "    for batch in dataloader:\n",
    "        print(batch[\"input_ids\"].shape)  # Check batch shape\n",
    "        break  # Remove in final implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25646baf-d557-4d86-af0b-d718309ed42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Big Data Solution - Apache Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"CrosswordProcessing\").getOrCreate()\n",
    "\n",
    "# Load large crossword dataset\n",
    "df_spark = spark.read.csv(\"crossword_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess: Clean and normalize text in parallel\n",
    "df_spark = df_spark.withColumn(\"Clue\", lower(col(\"Clue\")))\n",
    "df_spark = df_spark.withColumn(\"Clue\", regexp_replace(col(\"Clue\"), \"[^\\w\\s]\", \"\"))\n",
    "\n",
    "# Convert Spark DataFrame to Pandas if needed\n",
    "df_pandas = df_spark.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a3bda-be9d-484d-bb3a-08eec827993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real Time Crossword Solving: Kafka + Spark Streaming \n",
    "from confluent_kafka import Producer\n",
    "\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "producer.produce('crossword-clues', key=\"clue\", value=\"Capital of France\")\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182deef-0cb0-409f-85ca-9046187dff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "\n",
    "consumer = Consumer({'bootstrap.servers': 'localhost:9092', 'group.id': 'clue_solver', 'auto.offset.reset': 'earliest'})\n",
    "consumer.subscribe(['crossword-clues'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)  # Wait for new crossword clues\n",
    "    if msg is None:\n",
    "        continue\n",
    "    clue = msg.value().decode(\"utf-8\")\n",
    "    \n",
    "    # Solve clue using BERT\n",
    "    tokens = tokenizer(clue, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    \n",
    "    predicted_label = torch.argmax(output.logits, dim=1).item()\n",
    "    predicted_answer = label_encoder.inverse_transform([predicted_label])\n",
    "    \n",
    "    print(f\"Clue: {clue} | Predicted Answer: {predicted_answer[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d48e7-b19e-4205-851b-6791a83f9351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exploring the Full Dataset \n",
    "- Questions\n",
    "    - How often are answered reused? - If answers are reused frequently, then we can reuse clues!\n",
    "    - Identify trends over the last few decades in NYT crosswords\n",
    "        - Can use my clue classification algo. to breakdown every crossword\n",
    "    - **Can I make my own difficulty rating?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a7ff4f-26ec-4074-816a-5ba85753fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 781539 entries, 0 to 781572\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype         \n",
      "---  ------  --------------   -----         \n",
      " 0   Date    781539 non-null  datetime64[ns]\n",
      " 1   Word    781539 non-null  object        \n",
      " 2   Clue    781539 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2)\n",
      "memory usage: 23.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Basic Loading and Cleaning Again\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result['encoding'])  # Displays the detected encoding\n",
    "\n",
    "df = pd.read_csv('nytcrosswords.csv', encoding=result['encoding'])\n",
    "\n",
    "#drop any null rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#convert date col to datetimetype \n",
    "df['Date'] = pd.to_datetime(df['Date'], format = '%m/%d/%Y')\n",
    "\n",
    "#Normalize clues and answers to account for any discrepancies \n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "df['Clue'] = df['Clue'].apply(clean_text)\n",
    "df['Word'] = df['Word'].apply(clean_text)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adfafa7a-b874-4b8e-9f94-363abb5594bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 781539 entries, 0 to 781572\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Word    781539 non-null  object\n",
      " 1   Clue    781539 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 17.9+ MB\n",
      "       Word  Count\n",
      "17700   era    634\n",
      "3639   area    534\n",
      "17728   ere    510\n",
      "38675   one    510\n",
      "16684   eli    493\n",
      "             Clue  Count\n",
      "0                    176\n",
      "216623    jai ___    122\n",
      "260733  mauna ___    113\n",
      "341628   put away    113\n",
      "337235    pro ___    106\n",
      "                Clue  Unique_Answers\n",
      "0                                109\n",
      "369084   see 1across              57\n",
      "369073  see 17across              52\n",
      "277342          name              41\n",
      "397407         split              38\n"
     ]
    }
   ],
   "source": [
    "#Group by, look for duplicates\n",
    "#Drop date col for now\n",
    "df1 = df.iloc[:, 1:3]\n",
    "df1.info()\n",
    "#First groupby answer, should default to count \n",
    "# Group by 'Answer' by count, sort, and print\n",
    "df1 = df.groupby('Word').size().reset_index(name='Count')\n",
    "df1 = df1.sort_values(by='Count', ascending=False)\n",
    "print(df1.head())\n",
    "\n",
    "#Do same groupby for clues\n",
    "df2 = df.groupby('Clue').size().reset_index(name='Count')\n",
    "df2 = df2.sort_values(by = 'Count', ascending = False)\n",
    "print(df2.head())\n",
    "\n",
    "# Group by 'Clue' and count unique answers\n",
    "clue_group = df.groupby('Clue')['Word'].nunique().reset_index()\n",
    "clue_group.rename(columns={'Word': 'Unique_Answers'}, inplace=True)\n",
    "# Sort by number of unique answers\n",
    "clue_group = clue_group.sort_values(by='Unique_Answers', ascending=False)\n",
    "# View top results\n",
    "print(clue_group.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b94b3676-00b2-40ff-be32-2820d1405d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 296951 duplicate clues which is 38.0% of the total. There are 781539 total clues and 484588 unique clues.\n",
      "There are 296951 duplicate answers which is 91.9% of the total. There are 781539 and 63310 unique answers. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x177c2760050>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Naive way to do it \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "clues = df['Clue']\n",
    "total_clues = len(clues)\n",
    "uniq_clues = len(set(clues))\n",
    "diff_clues = total_clues - uniq_clues\n",
    "prop_clues = np.round( (1 -  (uniq_clues/total_clues)) * 100, 1)\n",
    "print(f\"There are {diff_clues} duplicate clues which is {prop_clues}% of the total. There are {total_clues} total clues and {uniq_clues} unique clues.\")\n",
    "\n",
    "answers = df['Word']\n",
    "total_answers = len(answers)\n",
    "uniq_answers = len(set(answers))\n",
    "diff_answers = total_answers - uniq_answers\n",
    "prop_answers = np.round((1 - (uniq_answers/total_answers)) * 100, 1)\n",
    "print(f\"There are {diff_clues} duplicate answers which is {prop_answers}% of the total. There are {total_answers} and {uniq_answers} unique answers. \")\n",
    "\n",
    "plt.plot(uniq_clues, uniq_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dace25e-4cbe-4a42-8596-dee6512a4ed2",
   "metadata": {},
   "source": [
    "### General Approach Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe0bb0-7a8d-4f62-92fc-8928911be193",
   "metadata": {},
   "source": [
    "- Provide various hints\n",
    "    - Synonym of answer\n",
    "    - Antonym of answer\n",
    "    - Help give context to the clue - sentiment analysis, text classification \n",
    "    - Help give to context to the answer\n",
    "        - What kind of word etc.\n",
    "    - Answer used in sentence\n",
    "    - Varying level of hints\n",
    "- How can I incorporate NLP?\n",
    "- Problem: some answers are multiple words/phrase/proper noun/name\n",
    "- Later quality of life stuff\n",
    "    - Autochecker\n",
    "    - Full puzzle checker\n",
    "    - Single word checker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d71d57-6c3a-4788-96f9-eefb70701dd3",
   "metadata": {},
   "source": [
    "### Classification of Clue Types \n",
    "- Goal: classify clue types as definition, wordplay, anagram, name/etc.\n",
    "- Necessary steps:\n",
    "    - Create labels for different clue types\n",
    "    - Train some classification program using labeled data\n",
    "        - Options: Naive Bayes from DS122, fine tune BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f92551-0bf4-43b4-8831-d14d8a9749fe",
   "metadata": {},
   "source": [
    "### Other Avenues of Exploration for similar words\n",
    "- WordNet --> directly pull synonyms, antonyms, related words\n",
    "- Thesaurus APIs --> fetch related words dynamically\n",
    "- Context-Aware Model --> pre-trained models like BERT to train a model to predict answers or generate hints based on clue embeddings\n",
    "    - Use hyperparameter fine tuning\n",
    "- Real-Time Suggestions -->   leverage APIs to fetch synonyms/related terms in real-time. Probably useful if we haven't seen the answer yet\n",
    "    - Use GPT APIs for generateing context-aware hint\n",
    "- Evaluation\n",
    "    - Try on clues not in the dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61417ffc-e2d3-4315-91b3-3dd69cb0b19c",
   "metadata": {},
   "source": [
    "### Hint Help 1: Similar Words/Synonyms/Antonyms/Related Words to Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e72428-8aca-440e-a782-7b73a0f543fc",
   "metadata": {},
   "source": [
    "- **Goal/Explanation**: \n",
    "- **Challenges**: How to handle Answers that are multiple words combined/made up words/names or pronoun/acronym\n",
    "    - Lots of possible edge cases for Crossword Answers:\n",
    "        - Multi-woerd answers --> lematize each word separately, rejoin them\n",
    "        - Made-up words/slang --> use original word if not recognized\n",
    "        - Proper nouns --> detect named entities, keep same\n",
    "        - Acronyms --> try to identify ...\n",
    "        - Foreign words --> keep unchanged\n",
    "        - Hyphenated words - keep if word exists\n",
    "        - Contractions --> expand/keep original\n",
    "        - Numbers in words --> keep \n",
    "- **Process**:\n",
    "- **Areas of improvement**\n",
    "    - Find more explicit ways to handle the edge cases, ie. use a super long list of common acronyms/slang etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e87f23-1b7c-4949-9903-e4d35d07856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23420 entries, 0 to 23419\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Date             23420 non-null  object\n",
      " 1   Word             23420 non-null  object\n",
      " 2   Clue             23420 non-null  object\n",
      " 3   Character Count  23420 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 732.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>Character Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8609</th>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>SHEBA</td>\n",
       "      <td>Yemen, in the Bible</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22811</th>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>ICEE</td>\n",
       "      <td>Brain freeze cause, maybe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22315</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>Watch it!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024</th>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>TROT</td>\n",
       "      <td>Moderate pace</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>ONS</td>\n",
       "      <td>Switch positions</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date   Word                       Clue  Character Count\n",
       "8609   2021-04-22  SHEBA        Yemen, in the Bible                5\n",
       "22811  2021-08-01   ICEE  Brain freeze cause, maybe                4\n",
       "22315  2021-09-30  VIDEO                  Watch it!                5\n",
       "9024   2021-10-08   TROT              Moderate pace                4\n",
       "888    2021-04-21    ONS           Switch positions                3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in cleaned data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('deep_learning_nytcrosswords2021.csv')\n",
    "df.info()\n",
    "\n",
    "#for now just pick small subset of data, since this section doesn't really require training \n",
    "df = df.sample(n=100)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46fa186-5139-4439-9a72-673ffb9653e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word Processed_Word\n",
      "8609   SHEBA          SHEBA\n",
      "22811   ICEE           ICEE\n",
      "22315  VIDEO          video\n",
      "9024    TROT           TROT\n",
      "888      ONS            ONS\n"
     ]
    }
   ],
   "source": [
    "#Additional preprocessing: more cleaning + lemmatization (reduce words to base/root form)\n",
    "import re #regular expression to remove undesired characters\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nlp = spacy.load(\"en_core_web_sm\") #spaCy English model for lemmatization + named entity recognition\n",
    "\n",
    "def is_acronym(word):\n",
    "    \"\"\"\n",
    "    Identifies if a word is an acronym or abbreviation based on:\n",
    "    - Short uppercase words (2-5 characters, no vowels in the middle) (e.g., \"FBI\", \"CIA\")\n",
    "    - Contains numbers (e.g., \"G7\", \"3D\")\n",
    "    - Hyphenated uppercase words (e.g., \"X-RAY\", \"T-SHIRT\")\n",
    "    - CamelCase words (e.g., \"iPhone\", \"eBay\")\n",
    "    - Named Entity Recognition (NER) detects organizations (e.g., \"NASA\", \"FBI\")\n",
    "    \"\"\"\n",
    "    word = word.strip()  # Remove extra spaces\n",
    "\n",
    "    # Short uppercase words (2-5 characters, minimal vowels) are likely acronyms\n",
    "    if len(word) <= 5 and not re.search(r'[aeiou]', word[1:].lower()):\n",
    "        return True  \n",
    "\n",
    "    # Allow hyphenated acronyms (X-RAY, T-SHIRT)\n",
    "    if \"-\" in word and all([w.isupper() for w in word.split(\"-\")]):\n",
    "        return True  \n",
    "\n",
    "    # Numeric acronyms (3D, B2B)\n",
    "    if re.search(r'\\d', word):\n",
    "        return True  \n",
    "\n",
    "    # Check CamelCase words (e.g., iPhone, eBay)\n",
    "    if re.search(r'^[a-z]+[A-Z]', word):\n",
    "        return True  \n",
    "\n",
    "    # Named Entity Recognition (ORG, GPE detection)\n",
    "    doc = nlp(word)\n",
    "    if doc[0].ent_type_ in [\"ORG\", \"GPE\"]:\n",
    "        return True  \n",
    "\n",
    "    return False  # Otherwise, assume it's a normal word\n",
    "\n",
    "def clean_answers(text):\n",
    "    \"\"\"\n",
    "    Cleans and lemmatizes crossword answers while handling all major edge cases:\n",
    "    - Keeps acronyms & abbreviations (e.g., \"NASA\", \"FBI\", \"G7\") unchanged\n",
    "    - Preserves proper nouns (e.g., \"EINSTEIN\", \"BOSTON\")\n",
    "    - Lemmatizes multi-word answers correctly\n",
    "    - Keeps made-up words/slang unchanged if lemmatization fails\n",
    "    - Maintains hyphenated words (e.g., \"SELF-MADE\", \"X-RAY\")\n",
    "    - Prevents foreign words from being altered incorrectly\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return text  # Return as-is if empty or not a string (avoids errors)\n",
    "\n",
    "    text = text.strip()  # Remove spaces\n",
    "    text = re.sub(r\"[^\\w\\s\\-’']\", '', text)  # Remove punctuation except hyphens & apostrophes\n",
    "\n",
    "    doc = nlp(text)  # Tokenize & process with spaCy\n",
    "    \n",
    "    processed_words = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue  # Skip stop words and punctuation\n",
    "        \n",
    "        # Keep acronyms, numbers, and uppercase abbreviations unchanged\n",
    "        if is_acronym(token.text) or token.is_digit:\n",
    "            processed_words.append(token.text)  # Keep as-is\n",
    "        elif token.ent_type_ in [\"PERSON\", \"GPE\", \"ORG\"]:  # Named Entities (Proper Nouns)\n",
    "            processed_words.append(token.text)  # Keep proper nouns as-is\n",
    "        else:\n",
    "            lemma = token.lemma_\n",
    "            processed_words.append(lemma if lemma != token.text else token.text)  # Use lemma if changed\n",
    "    \n",
    "    return \" \".join(processed_words)  # Do NOT convert everything to uppercase, keep original format\n",
    "\n",
    "# Create a new column for processed answers without modifying the original column\n",
    "df['Processed_Word'] = df['Word'].apply(clean_answers)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[['Word', 'Processed_Word']].head())\n",
    "#takes 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2f9c91-f992-4bfa-be84-c6fe37289882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we perform answer classification BEFORE embeddings so that we can more accurately determine similar words\n",
    "#Categorize answers into bins: \n",
    "\n",
    "# Define a list of words that spaCy frequently misclassifies as ORG\n",
    "FALSE_ORGS = {\"HAHAHA\", \"SLY\", \"AMMO\", \"OWE\", \"UNDEROATH\", \"AXL\"}\n",
    "\n",
    "def is_acronym(word):\n",
    "    \"\"\"Returns True if the word is an acronym.\"\"\"\n",
    "    return word.isupper() and len(word) <= 5  # Consider short uppercase words as acronyms\n",
    "\n",
    "def is_common_noun(word):\n",
    "    \"\"\"Check if a word is a common noun using WordNet.\"\"\"\n",
    "    synsets = wordnet.synsets(word.lower())\n",
    "    return any(s.pos() == 'n' for s in synsets)  # If it's a noun in WordNet, return True\n",
    "\n",
    "def classify_word_type(word):\n",
    "    \"\"\"Classifies words using spaCy's built-in NER & POS tagging with additional post-processing.\"\"\"\n",
    "    if not isinstance(word, str) or word.strip() == \"\":  # Handle empty or non-string values\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    doc = nlp(word.strip())\n",
    "    if not doc:  # Handle cases where spaCy fails to parse\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    token = doc[0]\n",
    "\n",
    "    # Named Entities (Prebuilt in spaCy)\n",
    "    if token.ent_type_ in [\"ORG\"]:\n",
    "        # Check if the word passes all the ORG checks\n",
    "        if word not in FALSE_ORGS and len(word) >= 5 and (is_acronym(word) or not word.islower()) and not is_common_noun(word):\n",
    "            return \"ORGANIZATION\"  # Only return \"ORGANIZATION\" if all checks pass\n",
    "    elif token.ent_type_ in [\"PERSON\"]:\n",
    "        return \"PERSON\"\n",
    "    elif token.ent_type_ in [\"GPE\", \"LOC\"]:  # **Combining GPE + LOC**\n",
    "        return \"LOCATION\"\n",
    "    \n",
    "    elif token.ent_type_ in [\"EVENT\"]:\n",
    "        return \"EVENT\"\n",
    "    elif token.ent_type_ in [\"WORK_OF_ART\"]:\n",
    "        return \"WORK_OF_ART\"\n",
    "    elif token.ent_type_ in [\"PRODUCT\"]:\n",
    "        return \"PRODUCT\"\n",
    "    elif token.ent_type_ in [\"DATE\"]:\n",
    "        return \"DATE\"\n",
    "    elif token.ent_type_ in [\"MONEY\"]:\n",
    "        return \"MONEY\"\n",
    "\n",
    "    # Parts of Speech (Prebuilt in spaCy)\n",
    "    if token.pos_ == \"VERB\":\n",
    "        return \"VERB\"\n",
    "    elif token.pos_ == \"NOUN\":\n",
    "        return \"NOUN\"\n",
    "    elif token.pos_ == \"ADJ\":\n",
    "        return \"ADJECTIVE\"\n",
    "    elif token.pos_ == \"ADV\":\n",
    "        return \"ADVERB\"\n",
    "\n",
    "    return \"UNKNOWN\"  # Default category for unknown types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e19c985c-3b0e-46f3-8586-3bc93568d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Extra check for multiple words using set of known words/dictionary\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Set of common English words (from nltk corpus)\n",
    "word_set = set(words.words())\n",
    "\n",
    "def is_multi_word(answer):\n",
    "    # We will now split the word at multiple positions and check if the parts are valid words\n",
    "    # Split the word into multiple parts and check against the dictionary\n",
    "    for i in range(1, len(answer)):\n",
    "        word1 = answer[:i]  # First part\n",
    "        word2 = answer[i:]  # Second part\n",
    "        \n",
    "        if word1 in word_set and word2 in word_set:\n",
    "            return True  # Valid split into two words\n",
    "    \n",
    "    # Try splitting into three parts\n",
    "    for i in range(1, len(answer)):\n",
    "        word1 = answer[:i]\n",
    "        for j in range(i+1, len(answer)):\n",
    "            word2 = answer[i:j]\n",
    "            word3 = answer[j:]\n",
    "            if word1 in word_set and word2 in word_set and word3 in word_set:\n",
    "                return True  # Valid split into three words\n",
    "\n",
    "    return False  # If no valid splits found, return False (single word)\n",
    "\n",
    "# Example usage\n",
    "print(is_multi_word('goback'))     # Expected output: True (split into \"go\" and \"back\")\n",
    "print(is_multi_word('draw'))       # Expected output: False (Single word)\n",
    "print(is_multi_word('art'))        # Expected output: False (Single word)\n",
    "print(is_multi_word('PlayGround')) # Expected output: True (split into \"Play\" and \"Ground\")\n",
    "print(is_multi_word('goahead'))    # Expected output: True (split into \"go\" and \"ahead\")\n",
    "print(is_multi_word('someotherword'))  # Expected output: False (Single word)\n",
    "print(is_multi_word('kickstartnew')) # Expected output: True (split into \"kick\", \"start\", and \"new\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b0772d-2521-4630-b5d1-e352461a63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra check for multiple words using tokenization\n",
    "def is_multi_word(answer):\n",
    "    # Tokenize the answer\n",
    "    doc = nlp(answer)\n",
    "    # Return True if the number of tokens (words) is greater than 1\n",
    "    return len(doc) > 1\n",
    "\n",
    "\n",
    "# Apply the function to check if the word is multi-word and create a new column\n",
    "df['is_multi_word'] = df['Word'].apply(is_multi_word)\n",
    "\n",
    "#Apply to cleaned/processed answer\n",
    "df[\"Word_Type\"] = df[\"Processed_Word\"].apply(classify_word_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4429e7-2a53-4ce9-b7e4-473a229921d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = is_multi_word('goback')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39400788-b363-4f80-8947-b73af75e734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "#Use word2vec model (google word dict. to convert words to vectors) to identify most similar words\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pretrained model\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fdf4939-7881-42a8-ba04-59fe63e483d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'running': ['rushing', 'sprinting', 'coming', 'switching']\n",
      "Similar words to 'Einstein': ['albert', 'corey', \"o'brien\", 'kristin']\n",
      "Similar words to 'New York': ['amy_zimmer_metro', 'brooklyn', 'patrick_arden_metro', 'los_angeles', 'denver']\n",
      "Similar words to 'jumping': ['leaping', 'climbing', 'leapt', 'hopping', 'bouncing']\n",
      "Similar words to 'fast': ['quick', 'rapidly', 'quickly', 'slow', 'rapid']\n",
      "Similar words to 'beautiful': ['gorgeous', 'lovely', 'wonderful', 'fabulous', 'loveliest']\n",
      "Similar words to 'Shakespeare': ['www.angelfire.com', 'home.htm', 'theatre.com', 'arts.org', 'nps.gov']\n",
      "Similar words to 'Einstein': ['albert', 'corey', \"o'brien\", 'kristin']\n",
      "Similar words to 'Beethoven': []\n",
      "Similar words to 'Paris': ['london', 'france', 'dubai', 'hilton', 'rome']\n",
      "Similar words to 'New York': ['amy_zimmer_metro', 'brooklyn', 'patrick_arden_metro', 'los_angeles', 'denver']\n",
      "Similar words to 'Tokyo': ['seoul', 'japan', 'toronto', 'washington', 'manhattan']\n",
      "Similar words to 'Mount Everest': ['everest', 'VESA_compatible', 'flexible_gooseneck', 'Amin_Brakk', 'Tye_Angland']\n",
      "Similar words to 'San Francisco': ['pa_ba', 'sama', 'Yaguchi']\n",
      "Similar words to 'Harry Potter': ['dawson', 'owen', 'ferguson', 'gordon', 'smith']\n",
      "Similar words to 'quantum': ['superpositions', 'fermionic']\n",
      "Similar words to 'alchemy': ['alchemist', 'alchemical', 'alchemic', 'necromancy', 'transmutations']\n",
      "Similar words to 'mythology': ['mythos', 'mythological', 'myth', 'lore', 'iconography']\n"
     ]
    }
   ],
   "source": [
    "#Now we use embeddings to find similar words - however to make sure these words are actually unique and somewhat match the types\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"Helper function that returns root of word\"\"\"\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_\n",
    "\n",
    "def detect_named_entity_type(word):\n",
    "    \"\"\"Helper function that detects the type of named entity (e.g., PERSON, GPE, ORG)\"\"\"\n",
    "    doc = nlp(word)\n",
    "    return doc[0].ent_type_ if doc[0].ent_type_ in [\"PERSON\", \"GPE\", \"ORG\"] else None\n",
    "\n",
    "def is_valid_word(word, answer_entity_type, answer_is_multi_word):\n",
    "    \"\"\"\n",
    "    Filters words based on context:\n",
    "    - Allows named entities only if the answer is also a named entity and matches type.\n",
    "    - Ensures that locations (GPE) return only other locations.\n",
    "    - Allows multi-word hints if the answer is multi-word.\n",
    "    - Prevents random garbage words from appearing.\n",
    "    \"\"\"\n",
    "    if \"_\" in word or \" \" in word:  # Allow multi-word hints only if answer is multi-word\n",
    "        return answer_is_multi_word\n",
    "    if re.search(r\"\\d\", word):  # Remove words containing numbers\n",
    "        return False\n",
    "\n",
    "    word_entity_type = detect_named_entity_type(word)\n",
    "\n",
    "    # If the answer is a GPE (location), ensure only locations appear\n",
    "    if answer_entity_type == \"GPE\" and word_entity_type != \"GPE\":\n",
    "        return False  # Remove non-location words when the answer is a location\n",
    "\n",
    "    # Only keep named entities that match the answer type\n",
    "    if word_entity_type and word_entity_type != answer_entity_type:\n",
    "        return False  # Remove mismatched named entities\n",
    "\n",
    "    # If the answer is a named entity, prefer similar proper nouns\n",
    "    if answer_entity_type and not word_entity_type:\n",
    "        return False  # Remove non-entities when answer is an entity\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_word2vec_similar_words(answer, top_n=5):\n",
    "    \"\"\"\n",
    "    Returns the top-N most similar words using Word2Vec embeddings.\n",
    "    - Handles named entities separately.\n",
    "    - Supports multi-word answers by averaging embeddings.\n",
    "    - Ensures all returned words have unique lemmas.\n",
    "    \"\"\"\n",
    "    answer = answer.lower()\n",
    "    lemma_answer = lemmatize_word(answer)  # Get root form of the answer\n",
    "    answer_entity_type = detect_named_entity_type(answer)  # Identify entity type (PERSON, GPE, ORG)\n",
    "    answer_is_multi_word = \" \" in answer or \"_\" in answer  # Check if answer is multi-word\n",
    "    words = answer.split()  # Split for multi-word handling\n",
    "\n",
    "    # Check if the full answer exists in Word2Vec\n",
    "    if answer in wv:\n",
    "        similar_words = wv.most_similar(answer, topn=top_n * 5)  # Get extra words in case of filtering\n",
    "    else:\n",
    "        # Handle multi-word phrases by averaging word embeddings\n",
    "        vectors = [wv[word] for word in words if word in wv]\n",
    "        if not vectors:\n",
    "            return []  # No valid words found\n",
    "\n",
    "        avg_vector = np.mean(vectors, axis=0)  # Compute mean embedding\n",
    "        similar_words = wv.similar_by_vector(avg_vector, topn=top_n * 5)\n",
    "\n",
    "    similar_words = [w[0] for w in similar_words]  # Extract words only\n",
    "\n",
    "    # Filter out garbage results and near-duplicates\n",
    "    cleaned_words = set()\n",
    "    final_words = []\n",
    "    seen_lemmas = set()  # Track lemmatized versions of words\n",
    "\n",
    "    for w in similar_words:\n",
    "        lemma_w = lemmatize_word(w)\n",
    "        if (\n",
    "            lemma_w != lemma_answer and answer not in w and lemma_answer not in w\n",
    "            and is_valid_word(w, answer_entity_type, answer_is_multi_word)\n",
    "        ):\n",
    "            if lemma_w not in seen_lemmas:  # Ensure unique lemmas\n",
    "                seen_lemmas.add(lemma_w)\n",
    "                cleaned_words.add(w)  # Keep original word, not just the lemma\n",
    "                final_words.append(w)\n",
    "\n",
    "    return final_words[:top_n]  # Return only the top-N valid words\n",
    "\n",
    "# Example Test Cases\n",
    "print(f\"Similar words to 'running': {get_word2vec_similar_words('running')}\")\n",
    "print(f\"Similar words to 'Einstein': {get_word2vec_similar_words('Einstein')}\")\n",
    "print(f\"Similar words to 'New York': {get_word2vec_similar_words('New York')}\")\n",
    "print(f\"Similar words to 'jumping': {get_word2vec_similar_words('jumping')}\")\n",
    "print(f\"Similar words to 'fast': {get_word2vec_similar_words('fast')}\")\n",
    "print(f\"Similar words to 'beautiful': {get_word2vec_similar_words('beautiful')}\")\n",
    "print(f\"Similar words to 'Shakespeare': {get_word2vec_similar_words('Shakespeare')}\")\n",
    "print(f\"Similar words to 'Einstein': {get_word2vec_similar_words('Einstein')}\")\n",
    "print(f\"Similar words to 'Beethoven': {get_word2vec_similar_words('Beethoven')}\")\n",
    "print(f\"Similar words to 'Paris': {get_word2vec_similar_words('Paris')}\")\n",
    "print(f\"Similar words to 'New York': {get_word2vec_similar_words('New York')}\")\n",
    "print(f\"Similar words to 'Tokyo': {get_word2vec_similar_words('Tokyo')}\")\n",
    "print(f\"Similar words to 'Mount Everest': {get_word2vec_similar_words('Mount Everest')}\")\n",
    "print(f\"Similar words to 'San Francisco': {get_word2vec_similar_words('San Francisco')}\")\n",
    "print(f\"Similar words to 'Harry Potter': {get_word2vec_similar_words('Harry Potter')}\")\n",
    "print(f\"Similar words to 'quantum': {get_word2vec_similar_words('quantum')}\")\n",
    "print(f\"Similar words to 'alchemy': {get_word2vec_similar_words('alchemy')}\")\n",
    "print(f\"Similar words to 'mythology': {get_word2vec_similar_words('mythology')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9913c4-918c-4f58-ba60-d80c420f0fc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hint Help 3: Classification of Clue Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58d420-a3ff-46ac-82ce-1cd747446aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification of Clue Types \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create labels for clue types (e.g., 0 = definition, 1 = anagram)\n",
    "df['ClueType'] = ...  # Add this column based on manual labeling\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Clue'], df['ClueType'], test_size=0.2)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict clue types\n",
    "y_pred = classifier.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5176f-8828-47ef-ba21-dc59dbb4a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall word vectors pointing in same direction are most similar\n",
    "#wv.most_similar('____')\n",
    "\n",
    "#Helper function to identify similarity between words, 1 = synonym, -1 = antonym, 0 = none\n",
    "def find_cosine(vec1, vec2):\n",
    "  # Scale vectors to both have unit length\n",
    "  unit_vec1 = vec1/np.linalg.norm(vec1)\n",
    "  unit_vec2 = vec2/np.linalg.norm(vec2)\n",
    "  # The dot product of unit vectors gives the cosine of their angle\n",
    "  return np.dot(unit_vec1,unit_vec2)\n",
    "\n",
    "#Getting sentence level vectors\n",
    "    #Naive approach - avg meaning vector \n",
    "    #more advanced - neural network with embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0f833-a050-4bb7-ae8a-51601e7de013",
   "metadata": {},
   "source": [
    "### Hint Help 4: Fine tune transformer (BERT) to give hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80347d1-ed23-49bc-a944-33c697476712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_csv('deep_learning_nytcrosswords2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4577b4cf-101c-4ad6-9df4-81e002d04654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-25</td>\n",
       "      <td>STYE</td>\n",
       "      <td>Eyelid affliction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>FUNK</td>\n",
       "      <td>\"I only got a seventh-grade education, but I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>COCOA</td>\n",
       "      <td>Warmer in the winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-26</td>\n",
       "      <td>CHEF</td>\n",
       "      <td>___ Boyardee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>SEEDIER</td>\n",
       "      <td>More like a dive bar or certain bread</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Word                                               Clue\n",
       "0  2021-10-25     STYE                                  Eyelid affliction\n",
       "1  2021-01-27     FUNK  \"I only got a seventh-grade education, but I h...\n",
       "2  2021-08-12    COCOA                               Warmer in the winter\n",
       "3  2021-10-26     CHEF                                       ___ Boyardee\n",
       "4  2021-08-08  SEEDIER              More like a dive bar or certain bread"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21490a49-069e-4d7e-a42b-fb8a76116f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "False\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Should return False (CUDA is for NVIDIA)\n",
    "print(torch.backends.mps.is_available())  # Check if Metal is available (Mac users)\n",
    "print(torch.cuda.device_count())  # Should show 1+ if using ROCm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b503cd3-22eb-4a0b-aaab-48990636c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a38a7917b024836a068b4a938cb9684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4beac404308e4395b81d01c533de7e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815451e53874cd2af03d86368307858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is ready to use!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"BERT is ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b700750a-7914-458b-bacd-42588aeb1b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique answers: 13193\n"
     ]
    }
   ],
   "source": [
    "# Count unique answers dynamically\n",
    "num_unique_answers = df[\"Word\"].nunique()\n",
    "print(f\"Number of unique answers: {num_unique_answers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8308dd6c-f0c4-46f9-b7a5-7c59c7ad311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#using uncased model for speed and performacne \n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_unique_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d094f3-dfde-4853-a5f8-48d11b8aaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the clues\n",
    "tokens = tokenizer(df[\"Clue\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert answers to numerical labels (assuming we have 5000 unique answers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df[\"Word\"])  # Converts text answers to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb919ca-afdf-45be-aa2c-286de3bb2a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CrosswordDataset at 0x16d8d5b4610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CrosswordDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokens.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = CrosswordDataset(tokens, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9c265d7-af87-43ad-9790-286eb9d386db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Input IDs: tensor([  101,  3239, 21273, 21358, 29301,  3258,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Label: tensor(11338)\n",
      "Decoded Clue: [CLS] eyelid affliction [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "#Check if it worked so far:\n",
    "# Get the first item from the dataset\n",
    "first_sample = dataset[0]  # This should return a dictionary\n",
    "\n",
    "# Print the keys in the sample\n",
    "print(first_sample.keys())\n",
    "\n",
    "# Print the actual contents of the sample\n",
    "print(\"Input IDs:\", first_sample[\"input_ids\"])\n",
    "print(\"Attention Mask:\", first_sample[\"attention_mask\"])\n",
    "print(\"Label:\", first_sample[\"labels\"])\n",
    "\n",
    "print(\"Decoded Clue:\", tokenizer.decode(first_sample[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10f964-2a82-4602-9a8c-8ece4ad05c66",
   "metadata": {},
   "source": [
    "# 📌 Fine-Tuning BERT for Crossword Solving\r\n",
    "\r\n",
    "## **1️⃣ Conceptual Overview**\r\n",
    "Fine-tuning BERT means **adapting a pre-trained language model** to specialize in **solving crossword clues**. Instead of training BERT from scratch, we **modify its last layers** so that it learns to map **crossword clues to correct answers**.\r\n",
    "\r\n",
    "🔹 **What we’re doing:**  \r\n",
    "- Giving BERT **crossword clues** as input.  \r\n",
    "- Training it to **predict the correct answer** (classification task).  \r\n",
    "- Using **supervised learning** (training with labeled crossword data).  \r\n",
    "- Adjusting BERT’s weights so it learns **patterns in crossword clues** over multiple epochs.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **2️⃣ Technical Breakdown**\r\n",
    "### **1️⃣ Loading Pre-trained BERT Model**\r\n",
    "- We use `bert-base-uncased`, a pre-trained **Transformer model** that already understands English.  \r\n",
    "- Modify BERT’s **final layer** to classify one of many possible crossword answers.\r\n",
    "\r\n",
    "### **2️⃣ Tokenizing Data**\r\n",
    "- Convert crossword clues into **tokenized input** that BERT can understand.  \r\n",
    "- Convert answers into **numerical labels** using `LabelEncoder()`.\r\n",
    "\r\n",
    "### **3️⃣ Training Process (Fine-Tuning)**\r\n",
    "The fine-tuning process consists of:\r\n",
    "1. **Forward Pass:** BERT takes a **tokenized crossword clue** and predicts an answer.  \r\n",
    "2. **Loss Calculation:** Compare BERT’s predicted answer to the correct answer using **CrossEntropyLoss**.  \r\n",
    "3. **Backpropagation:** Compute gradients to understand **how much each weight contributed to the error**.  \r\n",
    "4. **Optimizer Update:** Adjust BERT’s weights using **Adam optimizer- Common optimization algo in DL. * to improve predictions.  \r\n",
    "5. **Repeat for Multiple Epochs:** The model gradually gets better at predicting correct answers.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **3️⃣ Key Code Components**\r\n",
    "```python\r\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)  # Adjust BERT’s weights\r\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Measure how far off the predictions are\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    for batch in dataloader:\r\n",
    "        optimizer.zero_grad()  # Reset gradients\r\n",
    "        outputs = model(**inputs)  # Forward pass: Predict crossword answer\r\n",
    "        loss = loss_fn(outputs.logits, labels)  # Calculate loss\r\n",
    "        loss.backward()  # Compute gradients\r\n",
    "        optimizer.step()  # Update model weights\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71ffda-3c5b-4251-8bdf-3ece9321eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to GPU if available\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3307d3-140f-449b-b17d-a293a8a53475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8778f8ec-be57-4075-8c8e-e61e85d3e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23420 entries, 0 to 23419\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   Date    23420 non-null  datetime64[ns]\n",
      " 1   Word    23420 non-null  object        \n",
      " 2   Clue    23420 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2)\n",
      "memory usage: 549.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4b8d3-3961-480f-9f26-44ac7a30ff03",
   "metadata": {},
   "source": [
    "### Crossword Inputs Section\n",
    "- Option 1: manually type in hint and the answer --> not a very good option since you have to see the answer but simple enough\n",
    "- Option 2: manually type in just the clue, spaces used --> more realistic scenario but helper has to come up with the answer.\n",
    "- Option 3: use computer vision to scan the crossword clues and the crossword answers.\n",
    "    - Easiest/best/fastest solution, but requires user to have an answer key and also to look at it.\n",
    "- Option 4: use computer vision to scan empty crossword with hints. Helper has to come up with the answers on its own.\n",
    "    - Probably the most practical for a normal person doing a newspaper crossword without access to answers.\n",
    "    - So this is the ideal end goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a471a4-1111-4b40-931e-21e4cef9f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

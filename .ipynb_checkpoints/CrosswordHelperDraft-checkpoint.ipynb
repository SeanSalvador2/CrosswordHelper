{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e35689-ea06-4f03-9a61-9e91717a1aee",
   "metadata": {},
   "source": [
    "### Project Introduction\n",
    "- **Goal**: The goal of this project is to create a crossword helper that provides the user additional hints/clues to make solving crosswords more enjoyable. For now, the crossword helper assumes that you have access to the correct answers, but the end product would not require this. Moreover, a separate computer vision piece is being developed so a user can just take a picture of the entire crossword and request help where needed. For now, the crossword will operate on a clue/answer pair as being the input. As far as hint generation goes, the project is heading in a few different directions with varying levels of complexity, which include but are not limited to:\n",
    "  1. Provide synonyms/related words/antonyms to the answer --> use embeddings/thesaurus\n",
    "  2. Provide answer classification so the user know what *kind* of word they should be thinking of --> classification problem, probably exists\n",
    "  3. Provide clue classification so the user knows what *kind* of hint they are looking at --> classification problem\n",
    "  4. Provide new additional hints so the user can look at an answer from a different perspective --> train a transformer?\n",
    "- Data: The data used in this project consists of NYT crossword data from 1993-2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826e67-d3dc-456b-b6e9-2c489d2ee754",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initial Data Inspection, Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874b76d-2fe4-48e1-9c51-6a4cb3b9c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result['encoding'])  # Displays the detected encoding\n",
    "\n",
    "df = pd.read_csv('nytcrosswords.csv', encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b4d5a-bcb7-4a00-b4b1-eeb6b9170eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Minimal Cleaning for Deep Learning \n",
    "#drop any null rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#simple cleaning - get rid of excess whitespace, let BERT handle the rest!\n",
    "df['Clue'] = df['Clue'].str.strip()\n",
    "df['Word'] = df['Word'].str.strip()\n",
    "df['Date'] = pd.to_datetime(df['Date'], format = '%m/%d/%Y')\n",
    "\n",
    "#Add character count column that shows the length of each answer\n",
    "df[\"Character Count\"] = df[\"Word\"].apply(len)\n",
    "\n",
    "#filter to 2021 for smaller dataset\n",
    "df = df[df['Date'].dt.year == 2021]\n",
    "\n",
    "#shuffle to reduce bias\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#let's add a column that tells you how many characters \n",
    "df.info()\n",
    "df.to_csv('deep_learning_nytcrosswords2021.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba30cda-f8bd-4776-87f1-a2263f1dd649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f980c81-d86a-4cdc-b1b6-dd9607581fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d40759-d85b-4186-867f-3095b5238cb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Advanced Data Loading - Batch Processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42803b-b78c-419e-b437-8f8ea9e66dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Smaller Data Solution - Pandas and Pytorch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Detect file encoding\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "# Define batch size and chunk size for efficient loading\n",
    "batch_size = 16\n",
    "chunk_size = 10000  # Adjust based on memory and performance\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define PyTorch Dataset class\n",
    "class CrosswordDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.clues = df[\"Clue\"].tolist()\n",
    "        self.answers = df[\"Word\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clue = self.clues[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        # Tokenize clue\n",
    "        encoding = tokenizer(clue, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": answer\n",
    "        }\n",
    "\n",
    "# Load CSV in chunks and process data in batches\n",
    "chunks = pd.read_csv('nytcrosswords.csv', encoding=encoding, chunksize=chunk_size)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Clean and filter data\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunk['Clue'] = chunk['Clue'].str.strip()\n",
    "    chunk['Word'] = chunk['Word'].str.strip()\n",
    "    chunk['Date'] = pd.to_datetime(chunk['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "    chunk = chunk[chunk['Date'].dt.year == 2021]\n",
    "\n",
    "    # Convert to PyTorch dataset and DataLoader\n",
    "    dataset = CrosswordDataset(chunk)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop (simplified example)\n",
    "    for batch in dataloader:\n",
    "        print(batch[\"input_ids\"].shape)  # Check batch shape\n",
    "        break  # Remove in final implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25646baf-d557-4d86-af0b-d718309ed42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Big Data Solution - Apache Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"CrosswordProcessing\").getOrCreate()\n",
    "\n",
    "# Load large crossword dataset\n",
    "df_spark = spark.read.csv(\"crossword_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess: Clean and normalize text in parallel\n",
    "df_spark = df_spark.withColumn(\"Clue\", lower(col(\"Clue\")))\n",
    "df_spark = df_spark.withColumn(\"Clue\", regexp_replace(col(\"Clue\"), \"[^\\w\\s]\", \"\"))\n",
    "\n",
    "# Convert Spark DataFrame to Pandas if needed\n",
    "df_pandas = df_spark.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a3bda-be9d-484d-bb3a-08eec827993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real Time Crossword Solving: Kafka + Spark Streaming \n",
    "from confluent_kafka import Producer\n",
    "\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "producer.produce('crossword-clues', key=\"clue\", value=\"Capital of France\")\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182deef-0cb0-409f-85ca-9046187dff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "\n",
    "consumer = Consumer({'bootstrap.servers': 'localhost:9092', 'group.id': 'clue_solver', 'auto.offset.reset': 'earliest'})\n",
    "consumer.subscribe(['crossword-clues'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)  # Wait for new crossword clues\n",
    "    if msg is None:\n",
    "        continue\n",
    "    clue = msg.value().decode(\"utf-8\")\n",
    "    \n",
    "    # Solve clue using BERT\n",
    "    tokens = tokenizer(clue, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    \n",
    "    predicted_label = torch.argmax(output.logits, dim=1).item()\n",
    "    predicted_answer = label_encoder.inverse_transform([predicted_label])\n",
    "    \n",
    "    print(f\"Clue: {clue} | Predicted Answer: {predicted_answer[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d48e7-b19e-4205-851b-6791a83f9351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exploring the Full Dataset \n",
    "- Questions\n",
    "    - How often are answered reused? - If answers are reused frequently, then we can reuse clues!\n",
    "    - Identify trends over the last few decades in NYT crosswords\n",
    "        - Can use my clue classification algo. to breakdown every crossword\n",
    "    - **Can I make my own difficulty rating?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7ff4f-26ec-4074-816a-5ba85753fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Loading and Cleaning Again\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "with open('nytcrosswords.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result['encoding'])  # Displays the detected encoding\n",
    "\n",
    "df = pd.read_csv('nytcrosswords.csv', encoding=result['encoding'])\n",
    "\n",
    "#drop any null rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#convert date col to datetimetype \n",
    "df['Date'] = pd.to_datetime(df['Date'], format = '%m/%d/%Y')\n",
    "\n",
    "#Normalize clues and answers to account for any discrepancies \n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "df['Clue'] = df['Clue'].apply(clean_text)\n",
    "df['Word'] = df['Word'].apply(clean_text)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfafa7a-b874-4b8e-9f94-363abb5594bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by, look for duplicates\n",
    "#Drop date col for now\n",
    "df1 = df.iloc[:, 1:3]\n",
    "df1.info()\n",
    "#First groupby answer, should default to count \n",
    "# Group by 'Answer' by count, sort, and print\n",
    "df1 = df.groupby('Word').size().reset_index(name='Count')\n",
    "df1 = df1.sort_values(by='Count', ascending=False)\n",
    "print(df1.head())\n",
    "\n",
    "#Do same groupby for clues\n",
    "df2 = df.groupby('Clue').size().reset_index(name='Count')\n",
    "df2 = df2.sort_values(by = 'Count', ascending = False)\n",
    "print(df2.head())\n",
    "\n",
    "# Group by 'Clue' and count unique answers\n",
    "clue_group = df.groupby('Clue')['Word'].nunique().reset_index()\n",
    "clue_group.rename(columns={'Word': 'Unique_Answers'}, inplace=True)\n",
    "# Sort by number of unique answers\n",
    "clue_group = clue_group.sort_values(by='Unique_Answers', ascending=False)\n",
    "# View top results\n",
    "print(clue_group.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b3676-00b2-40ff-be32-2820d1405d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive way to do it \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "clues = df['Clue']\n",
    "total_clues = len(clues)\n",
    "uniq_clues = len(set(clues))\n",
    "diff_clues = total_clues - uniq_clues\n",
    "prop_clues = np.round( (1 -  (uniq_clues/total_clues)) * 100, 1)\n",
    "print(f\"There are {diff_clues} duplicate clues which is {prop_clues}% of the total. There are {total_clues} total clues and {uniq_clues} unique clues.\")\n",
    "\n",
    "answers = df['Word']\n",
    "total_answers = len(answers)\n",
    "uniq_answers = len(set(answers))\n",
    "diff_answers = total_answers - uniq_answers\n",
    "prop_answers = np.round((1 - (uniq_answers/total_answers)) * 100, 1)\n",
    "print(f\"There are {diff_clues} duplicate answers which is {prop_answers}% of the total. There are {total_answers} and {uniq_answers} unique answers. \")\n",
    "\n",
    "plt.plot(uniq_clues, uniq_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dace25e-4cbe-4a42-8596-dee6512a4ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### General Approach Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe0bb0-7a8d-4f62-92fc-8928911be193",
   "metadata": {},
   "source": [
    "- Provide various hints\n",
    "    - Synonym of answer\n",
    "    - Antonym of answer\n",
    "    - Help give context to the clue - sentiment analysis, text classification \n",
    "    - Help give to context to the answer\n",
    "        - What kind of word etc.\n",
    "    - Answer used in sentence\n",
    "    - Varying level of hints\n",
    "- How can I incorporate NLP?\n",
    "- Problem: some answers are multiple words/phrase/proper noun/name\n",
    "- Later quality of life stuff\n",
    "    - Autochecker\n",
    "    - Full puzzle checker\n",
    "    - Single word checker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d71d57-6c3a-4788-96f9-eefb70701dd3",
   "metadata": {},
   "source": [
    "### Classification of Clue Types \n",
    "- Goal: classify clue types as definition, wordplay, anagram, name/etc.\n",
    "- Necessary steps:\n",
    "    - Create labels for different clue types\n",
    "    - Train some classification program using labeled data\n",
    "        - Options: Naive Bayes from DS122, fine tune BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f92551-0bf4-43b4-8831-d14d8a9749fe",
   "metadata": {},
   "source": [
    "### Other Avenues of Exploration for similar words\n",
    "- WordNet --> directly pull synonyms, antonyms, related words\n",
    "- Thesaurus APIs --> fetch related words dynamically\n",
    "- Context-Aware Model --> pre-trained models like BERT to train a model to predict answers or generate hints based on clue embeddings\n",
    "    - Use hyperparameter fine tuning\n",
    "- Real-Time Suggestions -->   leverage APIs to fetch synonyms/related terms in real-time. Probably useful if we haven't seen the answer yet\n",
    "    - Use GPT APIs for generateing context-aware hint\n",
    "- Evaluation\n",
    "    - Try on clues not in the dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61417ffc-e2d3-4315-91b3-3dd69cb0b19c",
   "metadata": {},
   "source": [
    "### Hint Help Feature 1: Answer Classification and Similar Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1bee6-0c47-43bf-a517-9040795ce185",
   "metadata": {},
   "source": [
    "#### Goal/Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e72428-8aca-440e-a782-7b73a0f543fc",
   "metadata": {},
   "source": [
    "- **Goal**: \n",
    "- **Methods**: Answer Classification Model #1 --> Using GlINER\n",
    "    - **Why GLINER** - Generalist and Lightweight NER --> designed to recognize entities beyond typical predefined categories. Wide array of entities. Allows for flexible/customizable labels!\n",
    "    - CAN ALSO BE USED FOR HINT CLASSIFICATION!!!\n",
    "    - **NEW IDEA** - Further **improve NER** by using GLiNER on the hints too! Combine inputs so we can be sure what the answer is!\n",
    "    - This way we can also maybe reverse engineer the answer/generate hints and maybe come up with some new model with stuff we can train on \n",
    "    - Other models used: Spacy, Roberta, Hybrid Spacy + Berta --> too many None categorizations which aren't really helpful (due to limited entity choice)\n",
    "    - https://github.com/urchade/GLiNER/blob/main/README.md\n",
    "- **Challenges**: How to handle Answers that are multiple words combined/made up words/names or pronoun/acronym\n",
    "    - Lots of possible edge cases for Crossword Answers:\n",
    "        - Multi-woerd answers --> lematize each word separately, rejoin them\n",
    "        - Made-up words/slang --> use original word if not recognized\n",
    "        - Proper nouns --> detect named entities, keep same\n",
    "        - Acronyms --> try to identify ...\n",
    "        - Foreign words --> keep unchanged\n",
    "        - Hyphenated words - keep if word exists\n",
    "        - Contractions --> expand/keep original\n",
    "        - Numbers in words --> keep \n",
    "- **Areas of improvement**\n",
    "    - Find more explicit ways to handle the edge cases, ie. use a super long list of common acronyms/slang etc.\n",
    "- **NEW IDEAS**:\n",
    "    - One shot/few shot learning/prompting --> have the model do some task/question its never seen\n",
    "    - Use knowledge graphs/RAG/other things\n",
    "    - Other learning approaches to consider\n",
    "        - Reinforcement learning -->\n",
    "        - Self-supervised\n",
    "        - Semi-supervised\n",
    "    - FOR HINTS\n",
    "        - Look for fill in the blanks with ___ and then use a mask model!\n",
    "- huggingface pipeline function + quick tour: https://huggingface.co/docs/transformers/en/quicktour#trainer---a-pytorch-optimized-training-loop\n",
    "    - NER - persons/organizations/locations in a sentence\n",
    "        - classify each word in a sentence:\n",
    "        - can you customize labels?\n",
    "    - Maybe token classification instead???\n",
    "        - NER/POS  \n",
    "    - sentiment analyis\n",
    "    - zero shot classificaiton - tries to label given whateber labels you want\n",
    "        - Can use for hints/clues/predictions\n",
    "        - Find the best models\n",
    "    - text generation --> finishes some prompt using predicted words. Get some max length. Get as many return sequences as you want.\n",
    "    - Fill mask - predicts what words goes in the blank (mask) and returns score/token/token_str. Get top k answers\n",
    "    - question-answering\n",
    "        - give question\n",
    "        - give context!\n",
    "- **Current Areas of Improvement**: The POS tagging is not good. Can also improve word embedding section to use knowledge graphs to get relationships for some answers/clues.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05438ff4-3f3b-4524-89a2-52646aa5b606",
   "metadata": {},
   "source": [
    "##### Zero Shot Models\n",
    "BART: BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder pre-trained on a large corpus. It is beneficial for generating textual data and has shown promising results in zero-shot classification tasks.\n",
    "T5: T5 (Text-to-Text Transfer Transformer) is a transformer model that frames almost all NLP tasks as text-to-text problems. It can be adapted for zero-shot learning by providing the task description as input alongside the text to classify.\n",
    "GPT-3: GPT-3 (Generative Pre-trained Transformer 3) is one of the most significant language models available and has impressive zero-shot capabilities. Although GPT-3 might not be directly accessible due to its size, smaller versions and similar models are available.\n",
    "RoBERTa: RoBERTa (A Robustly Optimized BERT Pre-training Approach) is a variant of BERT that modifies the training process to improve its performance. It is widely used for various NLP tasks, including zero-shot classification.\n",
    "BERT: BERT (Bidirectional Encoder Representations from Transformers) is one of the pioneering language models for NLP. While not explicitly designed for zero-shot learning, it can still perform reasonably well in zero-shot classification tasks.\n",
    "ALBERT: ALBERT (A Lite BERT) is a lightweight version of BERT that reduces the model‚Äôs size and training time while maintaining performance. It can be a good choice for zero-shot classification in resource-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e14dc7-bd2a-48d6-84ff-aab11c245061",
   "metadata": {},
   "source": [
    "##### Zero Shot Alternatives\n",
    "Alternatives to zero-shot learning\n",
    "Several alternative approaches to zero-shot learning exist for classification tasks. These methods vary in their complexity, data requirements, and performance. Some common alternatives include:\n",
    "\n",
    "Supervised Learning: A model is trained on a labelled dataset with examples for each class it needs to classify. This is the traditional approach to classification and is highly effective when a sufficient amount of labelled training data is available for all classes.\n",
    "Few-Shot Learning: Few-shot learning lies between zero-shot and fully supervised learning. It aims to classify data with only a few examples for each class. This approach is advantageous when labelled data is scarce for certain classes but available for others.\n",
    "Semi-Supervised Learning: Semi-supervised learning combines labelled and unlabeled data during training. It can leverage labelled examples for some classes and unlabeled data to improve classification performance.\n",
    "Transfer Learning: Transfer learning involves pre-training a model on a large dataset and then fine-tuning it on a smaller labelled dataset specific to the target task. This approach can be practical when the pre-trained model captures relevant features useful for the classification task.\n",
    "Multi-Task Learning: In multi-task learning, a single model is trained to perform multiple related tasks simultaneously. By leveraging knowledge from other related tasks, it can help improve classification performance.\n",
    "Active Learning: Active learning is an iterative approach where the model actively selects the most informative instances for labelling. This reduces the need for large amounts of labelled data and can improve classification performance with a smaller labelled dataset.\n",
    "Ensemble Methods: Ensemble methods combine predictions from multiple models to obtain more accurate and robust classifications. They can be used to improve classification performance when individual models might struggle to handle specific classes.\n",
    "Domain Adaptation: Domain adaptation aims to transfer knowledge from a source domain with labelled data to a target domain with different characteristics but lacks labelled data. It can be helpful when the target domain has another distribution from the source domain.\n",
    "Meta-Learning: Meta-learning, also known as ‚Äúlearning to learn,‚Äù trains a model to learn how to adapt quickly to new tasks with limited data. It can help handle new classes with only a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f22914-1b5c-47e7-bd9d-a70dd3d590a5",
   "metadata": {},
   "source": [
    "**GLiNER Notes**\n",
    "- Word-level models work better for finding multi-word entities, highlighting sentences or paragraphs. They require additional output postprocessing that can be found in the corresponding model card.\n",
    "- GLiNER NuNerZero: numind/NuNER_Zero (MIT) - +3% more powerful GLiNER Large v2.1, better suitable to detect multi-word entities\n",
    "- GLiNER NuNerZero 4k context: numind/NuNER_Zero-4k (MIT) - 4k-long-context NuNerZero\n",
    "\n",
    "\n",
    "- üî¨ Domain Specific Models\n",
    "- Personally Identifiable Information: üîç urchade/gliner_multi_pii-v1 (Apache 2.0)\n",
    "This model is capable of recognizing various types of personally identifiable information (PII), including but not limited to these entity types: person, organization, phone number, address, passport number, email, credit card number, social security number, health insurance id number, date of birth, mobile phone number, bank account number, medication, cpf, driver's license number, tax identification number, medical condition, identity card number, national id number, ip address, email address, iban, credit card expiration date, username, health insurance number, registration number, student id number, insurance number, flight number, landline phone number, blood type, cvv, reservation number, digital signature, social media handle, license plate number, cnpj, postal code, passport_number, serial number, vehicle registration number, credit card brand, fax number, visa number, insurance company, identity document number, transaction number, national health insurance number, cvc, birth certificate number, train ticket number, passport expiration date, and social_security_number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a5f0a-3751-40ae-9506-e1523b71d743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data/Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6e87f23-1b7c-4949-9903-e4d35d07856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23420 entries, 0 to 23419\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Date             23420 non-null  object\n",
      " 1   Word             23420 non-null  object\n",
      " 2   Clue             23420 non-null  object\n",
      " 3   Character Count  23420 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 732.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Load in cleaned data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('deep_learning_nytcrosswords2021.csv')\n",
    "df.info()\n",
    "\n",
    "#for now just pick small subset of data, since this section doesn't really require training \n",
    "df = df.sample(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7deffc-ccfc-42bd-92ba-288c10cc5f04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data Preprocessing and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de501e49-bc14-487b-940f-048c872d1de8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "**Explanation of the Code:**\n",
    "- Purpose: This section processes crossword answers to clean, categorize, and enhance them with NLP techniques. The goal is to prepare the data for generating related words and analyzing patterns in crossword clues.\n",
    "  \n",
    "- Key Steps:\n",
    "  1. Load & Preprocess Crossword Data  \n",
    "     - Loads a dataset of crossword clues and answers.  \n",
    "     - Uses a small subset (`n=100`) for quick processing.  \n",
    "     - Converts answers to lowercase for consistency.\n",
    "\n",
    "  2. Load NLP Models & Tools  \n",
    "     - Uses `spaCy` for text processing and part-of-speech (POS) tagging.  \n",
    "     - `PunctuationModel` restores capitalization & punctuation.  \n",
    "     - `wordsegment` helps correct improperly formatted multi-word answers.  \n",
    "     - `GLiNER` classifies answers into categories (e.g., Person, Place, Food, Science).  \n",
    "\n",
    "  3. Define Helper Functions  \n",
    "     - `restore_spacing(word)` ‚Üí Fixes spacing for multi-word answers.  \n",
    "     - `detect_multi_word(word)` ‚Üí Identifies if an answer has multiple words.  \n",
    "     - `classify_pos(word)` ‚Üí Tags the part-of-speech using `spaCy`.  \n",
    "     - `classify_with_gliner(answer, clue)` ‚Üí Uses GLiNER to assign semantic categories.  \n",
    "     - `lemmatize_word(word)` ‚Üí Converts words to their base form (e.g., \"running\" ‚Üí \"run\").  \n",
    "\n",
    "  4. Process the Crossword Dataset  \n",
    "     - Applies all the above functions to clean and enrich the dataset.  \n",
    "     - Restores punctuation, fixes multi-word formatting, performs POS tagging, and classifies words.  \n",
    "\n",
    "  5. Display Processed Data  \n",
    "     - Shows the first few rows of the cleaned and categorized crossword data.  \n",
    "\n",
    "This structured approach ensures that crossword answers are in a useful format for further analysis, including word similarity and hint generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3c8e22e-b1d4-4267-98d6-970c142a26eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4df268b755047219d8450b283f77edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import warnings\n",
    "from transformers import pipeline\n",
    "from wordsegment import load, segment\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from gliner import GLiNER\n",
    "from gliner.multitask import GLiNERClassifier\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define category labels for GLiNER classification\n",
    "LABELS = [\n",
    "    \"Person\", \"Place\", \"Thing\", \"Animal\", \"Food\", \"Science\", \"Art\", \"Sport\",\n",
    "    \"History\", \"Literature\", \"Music\", \"Brand\", \"Abbreviation\", \"Acronym\",\n",
    "    \"Foreign\", \"Wordplay (Pun/Anagram/Homophone)\", \"Mythology\", \"Religion\", \"Vehicle\", \"Clothing\",\n",
    "    \"Instrument\", \"Plant\", \"Event\", \"Concept\", \"Miscellaneous\",\n",
    "    \"Slang\", \"Geography\", \"Object\", \"Technology\", \"Expression\"\n",
    "]\n",
    "\n",
    "# Load NLP models once (global initialization for efficiency)\n",
    "print(\"Loading models...\")\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"parser\"])  # Transformer-based NLP model\n",
    "punctuation_model = PunctuationModel()  # Restores capitalization & punctuation\n",
    "load()  # Load word segmentation model\n",
    "\n",
    "# Load GLiNER multitask model for classification\n",
    "model_id = \"knowledgator/gliner-multitask-v1.0\"\n",
    "gliner_model = GLiNER.from_pretrained(model_id)\n",
    "classifier = GLiNERClassifier(model=gliner_model)\n",
    "print(\"Models loaded successfully!\")\n",
    "\n",
    "# Helper functions\n",
    "def restore_spacing(word):\n",
    "    \"\"\"Fixes spacing for improperly formatted words.\"\"\"\n",
    "    return \" \".join(segment(word.lower())).title()\n",
    "\n",
    "def detect_multi_word(word):\n",
    "    \"\"\"Detects if an answer consists of multiple words.\"\"\"\n",
    "    return \"MULTI-WORD\" if len(segment(word.lower())) > 1 and not word.islower() else \"SINGLE-WORD\"\n",
    "\n",
    "def classify_pos(word):\n",
    "    \"\"\"Tags part-of-speech (POS) using spaCy.\"\"\"\n",
    "    doc = nlp(word)\n",
    "    return \" \".join([token.pos_ for token in doc if token.pos_ in [\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"]]) if len(doc) > 1 else doc[0].pos_ if len(doc) > 0 else \"UNKNOWN\"\n",
    "\n",
    "def classify_with_gliner(answer, clue, top_n=3):\n",
    "    \"\"\"Classifies a clue-answer pair using GLiNER and returns the top N predicted labels.\"\"\"\n",
    "    formatted_text = f\"Clue: {clue}. Answer: {answer}\"\n",
    "    predictions = classifier(formatted_text, classes=LABELS, multi_label=True)\n",
    "    predictions = predictions[0] if isinstance(predictions, list) and len(predictions) > 0 and isinstance(predictions[0], list) else predictions\n",
    "    sorted_labels = sorted(predictions, key=lambda x: x[\"score\"], reverse=True)[:top_n]\n",
    "    return [f\"{label['label']} ({label['score']:.2f})\" for label in sorted_labels] if sorted_labels else [\"Other\"]\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"Lemmatizes words to their root form.\"\"\"\n",
    "    return nlp(word)[0].lemma_ if nlp(word) else word\n",
    "\n",
    "def process_crossword_data(csv_path, sample_size=100):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and processes a crossword dataset.\n",
    "    - Restores capitalization & punctuation\n",
    "    - Fixes improperly formatted multi-word entities\n",
    "    - Performs named entity recognition (NER) with GLiNER\n",
    "    - Classifies multi-word terms\n",
    "    - Performs POS tagging\n",
    "    - Lemmatizes words to their root form\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size)  # Use a smaller subset for faster processing\n",
    "    \n",
    "    df['Word'] = df['Word'].str.lower()  # Normalize to lowercase\n",
    "\n",
    "    print(\"Processing crossword data...\")\n",
    "    df[\"Fixed Word\"] = df[\"Word\"].apply(lambda x: punctuation_model.restore_punctuation(x))\n",
    "    df[\"Spaced Word\"] = df[\"Fixed Word\"].apply(restore_spacing)\n",
    "    df[\"GLiNER Labels\"] = df.apply(lambda row: classify_with_gliner(row[\"Spaced Word\"], row[\"Clue\"]), axis=1)\n",
    "    df[\"Multi Word\"] = df[\"Spaced Word\"].apply(detect_multi_word)\n",
    "    df[\"POS Tag\"] = df[\"Spaced Word\"].apply(classify_pos)\n",
    "    df[\"Lemmatized Word\"] = df[\"Spaced Word\"].apply(lemmatize_word)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46fa7caf-b3fb-4da3-82f6-c9b5ba39a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from deep_learning_nytcrosswords2021.csv...\n",
      "Processing crossword data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>Character Count</th>\n",
       "      <th>Fixed Word</th>\n",
       "      <th>Spaced Word</th>\n",
       "      <th>GLiNER Labels</th>\n",
       "      <th>Multi Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Lemmatized Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>tara</td>\n",
       "      <td>Figure skater Lipinski</td>\n",
       "      <td>4</td>\n",
       "      <td>tara.</td>\n",
       "      <td>Tara</td>\n",
       "      <td>[Sport (0.98), Person (0.96)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Tara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>2021-02-05</td>\n",
       "      <td>nepal</td>\n",
       "      <td>Home of many a Sherpa</td>\n",
       "      <td>5</td>\n",
       "      <td>nepal.</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>[other (1.00)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17684</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>moreso</td>\n",
       "      <td>On a larger scale</td>\n",
       "      <td>6</td>\n",
       "      <td>moreso.</td>\n",
       "      <td>More So</td>\n",
       "      <td>[History (0.68), Music (0.67), Literature (0.65)]</td>\n",
       "      <td>MULTI-WORD</td>\n",
       "      <td>ADV ADV</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18587</th>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>pecan</td>\n",
       "      <td>Pie nut</td>\n",
       "      <td>5</td>\n",
       "      <td>pecan.</td>\n",
       "      <td>Pecan</td>\n",
       "      <td>[Food (0.98), Science (0.58)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Pecan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>highc</td>\n",
       "      <td>An alto probably can't hit it</td>\n",
       "      <td>5</td>\n",
       "      <td>highc.</td>\n",
       "      <td>High C</td>\n",
       "      <td>[Music (0.97)]</td>\n",
       "      <td>MULTI-WORD</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15148</th>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>ents</td>\n",
       "      <td>Docs treating vertigo</td>\n",
       "      <td>4</td>\n",
       "      <td>ents.</td>\n",
       "      <td>Ents</td>\n",
       "      <td>[Music (0.64), Science (0.64), History (0.62)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>2021-05-29</td>\n",
       "      <td>itdepends</td>\n",
       "      <td>\"Not always\"</td>\n",
       "      <td>9</td>\n",
       "      <td>itdepends.</td>\n",
       "      <td>It Depends</td>\n",
       "      <td>[Literature (0.65), History (0.63), Music (0.63)]</td>\n",
       "      <td>MULTI-WORD</td>\n",
       "      <td>VERB</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20151</th>\n",
       "      <td>2021-05-03</td>\n",
       "      <td>palau</td>\n",
       "      <td>Island nation in the western Pacific</td>\n",
       "      <td>5</td>\n",
       "      <td>palau.</td>\n",
       "      <td>Palau</td>\n",
       "      <td>[History (0.82)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Palau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>2021-06-13</td>\n",
       "      <td>plus</td>\n",
       "      <td>Not only that</td>\n",
       "      <td>4</td>\n",
       "      <td>plus.</td>\n",
       "      <td>Plus</td>\n",
       "      <td>[History (0.68), Music (0.65), Literature (0.59)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18488</th>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>solo</td>\n",
       "      <td>\"Star Wars\" pilot who, despite his name, flies...</td>\n",
       "      <td>4</td>\n",
       "      <td>solo.</td>\n",
       "      <td>Solo</td>\n",
       "      <td>[Literature (0.60)]</td>\n",
       "      <td>SINGLE-WORD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Solo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date       Word  \\\n",
       "4058   2021-06-08       tara   \n",
       "20951  2021-02-05      nepal   \n",
       "17684  2021-06-30     moreso   \n",
       "18587  2021-03-30      pecan   \n",
       "6161   2021-01-05      highc   \n",
       "15148  2021-01-14       ents   \n",
       "6638   2021-05-29  itdepends   \n",
       "20151  2021-05-03      palau   \n",
       "6176   2021-06-13       plus   \n",
       "18488  2021-03-02       solo   \n",
       "\n",
       "                                                    Clue  Character Count  \\\n",
       "4058                              Figure skater Lipinski                4   \n",
       "20951                              Home of many a Sherpa                5   \n",
       "17684                                  On a larger scale                6   \n",
       "18587                                            Pie nut                5   \n",
       "6161                       An alto probably can't hit it                5   \n",
       "15148                              Docs treating vertigo                4   \n",
       "6638                                        \"Not always\"                9   \n",
       "20151               Island nation in the western Pacific                5   \n",
       "6176                                       Not only that                4   \n",
       "18488  \"Star Wars\" pilot who, despite his name, flies...                4   \n",
       "\n",
       "       Fixed Word Spaced Word  \\\n",
       "4058        tara.        Tara   \n",
       "20951      nepal.       Nepal   \n",
       "17684     moreso.     More So   \n",
       "18587      pecan.       Pecan   \n",
       "6161       highc.      High C   \n",
       "15148       ents.        Ents   \n",
       "6638   itdepends.  It Depends   \n",
       "20151      palau.       Palau   \n",
       "6176        plus.        Plus   \n",
       "18488       solo.        Solo   \n",
       "\n",
       "                                           GLiNER Labels   Multi Word  \\\n",
       "4058                       [Sport (0.98), Person (0.96)]  SINGLE-WORD   \n",
       "20951                                     [other (1.00)]  SINGLE-WORD   \n",
       "17684  [History (0.68), Music (0.67), Literature (0.65)]   MULTI-WORD   \n",
       "18587                      [Food (0.98), Science (0.58)]  SINGLE-WORD   \n",
       "6161                                      [Music (0.97)]   MULTI-WORD   \n",
       "15148     [Music (0.64), Science (0.64), History (0.62)]  SINGLE-WORD   \n",
       "6638   [Literature (0.65), History (0.63), Music (0.63)]   MULTI-WORD   \n",
       "20151                                   [History (0.82)]  SINGLE-WORD   \n",
       "6176   [History (0.68), Music (0.65), Literature (0.59)]  SINGLE-WORD   \n",
       "18488                                [Literature (0.60)]  SINGLE-WORD   \n",
       "\n",
       "       POS Tag Lemmatized Word  \n",
       "4058     PROPN            Tara  \n",
       "20951    PROPN           Nepal  \n",
       "17684  ADV ADV            more  \n",
       "18587    PROPN           Pecan  \n",
       "6161      NOUN            High  \n",
       "15148     NOUN             ent  \n",
       "6638      VERB              it  \n",
       "20151    PROPN           Palau  \n",
       "6176     CCONJ            plus  \n",
       "18488    PROPN            Solo  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process the df:\n",
    "df = process_crossword_data(\"deep_learning_nytcrosswords2021.csv\", sample_size=100)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9134542-663b-4095-ada0-bfbfb343f6ea",
   "metadata": {},
   "source": [
    "#### Feature 1: Adding Similar/Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864eb125-0714-4e6a-9cfa-2286a1c75193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9369c48a-e05a-4eed-8844-d986342249e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae71d63-013f-4a8f-a3c0-0f4d6b38a29e",
   "metadata": {},
   "source": [
    "##### Gliner Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37735255-3c01-415c-b5fe-d884a6bc730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for 'Rook': ['Foreign Word (0.76)', 'Sport (0.67)', 'Person (0.66)']\n"
     ]
    }
   ],
   "source": [
    "#GLINER experiment with multiple categories\n",
    "labels = [\n",
    "    # Core categories\n",
    "    \"Person (Historical/Literary/Fictional)\", \n",
    "    \"Place (Geographic/Constructed)\", \n",
    "    \"Animal (Real/Mythical)\",\n",
    "    \"Food (Dish/Ingredient)\",\n",
    "    \"Science (Biology/Chemistry/Physics)\", \n",
    "    \"Art (Visual/Performing)\", \n",
    "    \"Sport (Game/Athlete/Equipment)\",\n",
    "    \"Literature (Book/Author/Character)\",\n",
    "    \"Music (Genre/Instrument/Song)\",\n",
    "    \"Brand (Company/Product)\",\n",
    "    \"Vehicle (Type/Brand)\",\n",
    "    \"Plant (Flower/Tree)\",\n",
    "    \"Event (Historical/Cultural)\",\n",
    "    \n",
    "    # Crossword-specific helpers\n",
    "    \"Abbreviation (Common/Initialism)\",\n",
    "    \"Foreign Word (Language-Specific)\",  # e.g., French, Latin\n",
    "    \"Wordplay (Pun/Anagram/Homophone)\", \n",
    "    \"Mythology (Deity/Creature)\",\n",
    "    \"Religion (Practice/Figure)\",\n",
    "    \"Concept (Abstract/Idea)\",\n",
    "    \"Object (Everyday/Tool)\",\n",
    "    \"Clothing (Type/Brand)\",\n",
    "    \"Slang/Colloquialism\",\n",
    "    \"Acronym (Pronounceable)\",  # e.g., NASA vs. FBI\n",
    "    \"Geography (Landform/Region)\",\n",
    "    \"Time (Unit/Historical Era)\",\n",
    "    \n",
    "    # Fallback\n",
    "    \"Miscellaneous\"\n",
    "]\n",
    "\n",
    "label_groups = [\n",
    "    # Group 1: Core entities\n",
    "    [\"Person (Historical/Literary/Fictional)\", \"Place (Geographic/Constructed)\", \"Animal (Real/Mythical)\"],\n",
    "    \n",
    "    # Group 2: Culture & Activities\n",
    "    [\"Art (Visual/Performing)\", \"Sport (Game/Athlete/Equipment)\", \"Music (Genre/Instrument/Song)\"],\n",
    "    \n",
    "    # Group 3: Abstract/Crossword-Specific\n",
    "    [\"Wordplay (Pun/Anagram/Homophone)\", \"Abbreviation (Common/Initialism)\", \"Foreign Word (Language-Specific)\"],\n",
    "    \n",
    "    # Group 4: Objects & Brands\n",
    "    [\"Brand (Company/Product)\", \"Vehicle (Type/Brand)\", \"Clothing (Type/Brand)\"],\n",
    "    \n",
    "    # Group 5: Science & Nature\n",
    "    [\"Science (Biology/Chemistry/Physics)\", \"Plant (Flower/Tree)\", \"Geography (Landform/Region)\"]\n",
    "]\n",
    "\n",
    "def classify_crossword_answer(text, label_groups, top_n=3, threshold=0.1):\n",
    "    all_entities = []\n",
    "    for group in label_groups:\n",
    "        entities = model.predict_entities(text, labels=group, threshold=threshold)\n",
    "        all_entities.extend(entities)\n",
    "    \n",
    "    # Deduplicate and sort\n",
    "    seen = set()\n",
    "    unique_entities = []\n",
    "    for ent in sorted(all_entities, key=lambda x: x[\"score\"], reverse=True):\n",
    "        key = (ent[\"text\"], ent[\"label\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_entities.append(ent)\n",
    "    \n",
    "    # Format for crossword hints\n",
    "    formatted = []\n",
    "    for ent in unique_entities[:top_n]:\n",
    "        label = ent[\"label\"].split(\" (\")[0]  # Simplify for output (e.g., \"Person\" instead of \"Person (Historical/Literary/Fictional)\")\n",
    "        formatted.append(f\"{label} ({ent['score']:.2f})\")\n",
    "    \n",
    "    return formatted if formatted else [\"Miscellaneous\"]\n",
    "\n",
    "clue = \"King in chess\"\n",
    "answer = \"Rook\"\n",
    "\n",
    "# Add context to help GLiNER resolve ambiguity\n",
    "context = f\"Clue: '{clue}' (Answer: '{answer}')\"\n",
    "result = classify_crossword_answer(context, label_groups)\n",
    "\n",
    "print(f\"Labels for '{answer}': {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76128a3b-67ff-4655-b53b-7d3de8b849c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Test Approach 1: Word Embeddings (word2vec, Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39400788-b363-4f80-8947-b73af75e734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "Models loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sean\n",
      "[nltk_data]     Salvador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Use word2vec model (google word dict. to convert words to vectors) to identify most similar words\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pretrained model\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "print('model loaded')\n",
    "\n",
    "#Also use wordnet for more structure/tighter \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print('Models loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851f3eb3-787f-4fce-8cef-7621c7c5a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing WordNet + Word2Vec Synonyms**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for 'Black Sea' ‚Üí []\n",
      "Synonyms for 'Einstein' ‚Üí ['mastermind', 'Albert Einstein', 'genius', 'brain', 'Einstein']\n",
      "Synonyms for 'Shakespeare' ‚Üí ['Shakspere', 'Bard of Avon', 'William Shakspere', 'Shakespeare', 'William Shakespeare']\n",
      "Synonyms for 'Nike' ‚Üí ['Nike']\n",
      "Synonyms for 'Amazon' ‚Üí ['virago', 'Amazon River', 'amazon', 'Amazon']\n"
     ]
    }
   ],
   "source": [
    "#Let's try word embeddings, with some filitering to make sure generated words are at least under the same category\n",
    "#first try wordnet then word2vec for more complexity\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gliner import GLiNER\n",
    "\n",
    "# Define labels\n",
    "labels = [\n",
    "    \"Person\", \"Place\", \"Thing\", \"Animal\", \"Food\", \"Science\", \"Art\", \"Sport\",\n",
    "    \"History\", \"Literature\", \"Music\", \"Brand\", \"Abbreviation\", \"Foreign\",\n",
    "    \"Wordplay\", \"Mythology\", \"Religion\", \"Vehicle\", \"Clothing\", \"Instrument\",\n",
    "    \"Plant\", \"Event\", \"Concept\", \"Miscellaneous\"\n",
    "]\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"Lemmatizes a word using spaCy.\"\"\"\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_ if doc else word\n",
    "\n",
    "def get_gliner_labels(gliner_labels):\n",
    "    \"\"\"Extracts clean category labels from GLiNER output (removes confidence scores).\"\"\"\n",
    "    return {label.split(\" (\")[0] for label in gliner_labels}  # Remove score (0.XX) part\n",
    "\n",
    "def predict_gliner_labels(word):\n",
    "    \"\"\"Runs GLiNER to predict entity labels for a word.\"\"\"\n",
    "    gliner_prediction = model_NER.predict_entities(word, labels, threshold=0.2)\n",
    "    return {entity[\"label\"] for entity in gliner_prediction} if gliner_prediction else set()\n",
    "\n",
    "def is_valid_synonym(word, answer_lemmas, seen_lemmas, answer_labels, word_labels):\n",
    "    \"\"\"\n",
    "    Determines if a synonym is valid based on:\n",
    "    - **Matching GLiNER labels** (must share at least one).\n",
    "    - **No duplicate lemmatized words** (answer or previous synonyms).\n",
    "    - **Ensuring variety in generated words**.\n",
    "    \"\"\"\n",
    "    word_lemma = lemmatize_word(word)\n",
    "\n",
    "    # ‚úÖ Must match at least one GLiNER label\n",
    "    if not answer_labels & word_labels:\n",
    "        return False  # No category overlap\n",
    "\n",
    "    # ‚úÖ Ensure uniqueness by checking lemmas\n",
    "    if word_lemma in answer_lemmas or word_lemma in seen_lemmas:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_wordnet_synonyms(word):\n",
    "    \"\"\"Fetches synonyms from WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace(\"_\", \" \"))  # Replace underscores with spaces\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_word2vec_synonyms(answer, gliner_labels, top_n=5):\n",
    "    \"\"\"\n",
    "    Generates synonyms using Word2Vec while ensuring:\n",
    "    - Labels match GLiNER categories.\n",
    "    - No duplicate lemmatized words.\n",
    "    - Multi-word handling via embedding averaging.\n",
    "    \"\"\"\n",
    "    answer = answer.lower()\n",
    "    answer_lemmas = {lemmatize_word(word) for word in answer.split()}  # Root forms of answer words\n",
    "    answer_labels = get_gliner_labels(gliner_labels)  # Get clean GLiNER labels\n",
    "    words = answer.split()\n",
    "\n",
    "    # ‚úÖ Try getting a direct match for the full phrase\n",
    "    if answer in wv:\n",
    "        similar_words = wv.most_similar(answer, topn=top_n * 5)  # Fetch extra words to filter\n",
    "    else:\n",
    "        # ‚úÖ If phrase is missing, average embeddings of individual words\n",
    "        valid_vectors = [wv[word] for word in words if word in wv]\n",
    "        if not valid_vectors:\n",
    "            return []  # No valid embeddings found\n",
    "\n",
    "        avg_vector = np.mean(valid_vectors, axis=0)  # Compute mean embedding\n",
    "        similar_words = wv.similar_by_vector(avg_vector, topn=top_n * 5)\n",
    "\n",
    "    similar_words = [w[0] for w in similar_words]  # Extract words only\n",
    "\n",
    "    # ‚úÖ Filtering: Keep only words that match at least **one** category and aren't duplicates\n",
    "    cleaned_words = set()\n",
    "    seen_lemmas = set()  # Track lemmas to avoid repetition\n",
    "\n",
    "    for word in similar_words:\n",
    "        word_labels = predict_gliner_labels(word)  # Classify Word2Vec word with GLiNER\n",
    "\n",
    "        if is_valid_synonym(word, answer_lemmas, seen_lemmas, answer_labels, word_labels):\n",
    "            lemma_word = lemmatize_word(word)\n",
    "            seen_lemmas.add(lemma_word)  # Prevent duplicates\n",
    "            cleaned_words.add(word)  # Keep original word\n",
    "\n",
    "        if len(cleaned_words) >= top_n:\n",
    "            break  # ‚úÖ Stop once we have enough valid words\n",
    "\n",
    "    return list(cleaned_words)\n",
    "\n",
    "def get_combined_synonyms(answer, gliner_labels, top_n=5):\n",
    "    \"\"\"\n",
    "    Combines **WordNet and Word2Vec** for better synonym generation.\n",
    "    - WordNet first (higher-quality synonyms).\n",
    "    - Word2Vec fills in gaps.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Step 1: Get WordNet synonyms\n",
    "    wordnet_synonyms = get_wordnet_synonyms(answer)\n",
    "\n",
    "    # ‚úÖ Step 2: Get Word2Vec synonyms (only if WordNet gave too few results)\n",
    "    if len(wordnet_synonyms) < top_n:\n",
    "        w2v_synonyms = get_word2vec_synonyms(answer, gliner_labels, top_n=top_n - len(wordnet_synonyms))\n",
    "    else:\n",
    "        w2v_synonyms = []\n",
    "\n",
    "    # ‚úÖ Combine results, ensuring uniqueness\n",
    "    combined_synonyms = list(set(wordnet_synonyms + w2v_synonyms))\n",
    "\n",
    "    return combined_synonyms[:top_n]  # Limit results\n",
    "\n",
    "# ‚úÖ **Example Test Cases**\n",
    "test_data = [\n",
    "    (\"Black Sea\", [\"Foreign Word (0.90)\", \"Geography (0.86)\", \"Place (0.82)\"]),\n",
    "    (\"Einstein\", [\"Person (0.98)\", \"Science (0.95)\"]),\n",
    "    (\"Shakespeare\", [\"Person (0.96)\", \"Literature (0.92)\"]),\n",
    "    (\"Nike\", [\"Brand (0.99)\", \"Sport (0.91)\"]),\n",
    "    (\"Amazon\", [\"Brand (0.92)\", \"Place (0.85)\"]),\n",
    "]\n",
    "\n",
    "print(\"\\nüîπ **Testing WordNet + Word2Vec Synonyms**\")\n",
    "for answer, labels in test_data:\n",
    "    print(f\"Synonyms for '{answer}' ‚Üí {get_combined_synonyms(answer, labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c34d4-39c7-49fd-bc00-02f610d6347d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Test Approach 2: Knowledge Graph's + ConceptNet for better relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eabf0feb-dae0-49ab-9789-a32aa8711f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing Expanded ConceptNet Synonyms & Related Terms**\n",
      "üîπ **ConceptNet results for 'Shakespeare':**\n",
      "   - Synonyms: ['a great dramatist']\n",
      "   - Related Terms: ['seventeenth', 'centuries', 'christopher marlowe', 'harold pinter', 'poet', 'shakespearian', 'macbeth', 'english', 'playwright', 'sonnets']\n",
      "\n",
      "üîπ **ConceptNet results for 'Einstein':**\n",
      "   - Synonyms: ['a physicist', 'genius', 'a very intelligent man']\n",
      "   - Related Terms: ['photon', 'stephen hawking', 'relativity', 'mole', 'genius', 'e mc', 'theoretical physicist', 'paul dirac', 'frequency', 'isaac newton']\n",
      "\n",
      "üîπ **ConceptNet results for 'Nike':**\n",
      "   - Synonyms: ['sneaks', 'information appliance']\n",
      "   - Related Terms: ['victory', 'victoria', 'asteroid', 'sneakers', 'tennis shoes', 'jordans', 'reebok', 'athena', 'triumph', 'adidas']\n",
      "\n",
      "üîπ **ConceptNet results for 'Black Sea':**\n",
      "   - Synonyms: ['black sea', 'sea']\n",
      "   - Related Terms: ['southeastern europe', 'caucasus', 'novorossiysk', 'euxinian', 'euxine', 'white sea', 'turkey', 'inland sea', 'yellow sea', 'red sea']\n",
      "\n",
      "üîπ **ConceptNet results for 'Amazon':**\n",
      "   - Synonyms: ['a river', 'video game', 'fictional female person', 'woman', 'mythical being', 'sentient animal', 'parrot']\n",
      "   - Related Terms: ['Peru', 'atlantic', 'xingu', 'south america', 'obliterate', 'black sea', 'overwhelm', 'igarap√©', 'maloca', 'brazil']\n",
      "\n",
      "üîπ **ConceptNet results for 'Physics':**\n",
      "   - Synonyms: ['a science', 'natural science', 'physical science']\n",
      "   - Related Terms: ['chemistry', 'psychic', 'energy', 'body', 'chromodynamics', 'matter', 'laws', 'atoms', 'quantum theory', 'scientific']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VERY VERY GOOD\n",
    "#Use all relevant relationships, #Mix of previous two, with related words + output diversity\n",
    "import requests\n",
    "import difflib\n",
    "\n",
    "def get_conceptnet_synonyms_and_related(word, top_n=10, weight_threshold=0.5, similarity_threshold=0.8):\n",
    "    \"\"\"Fetch synonyms and related words from ConceptNet while ensuring diversity and relevance.\"\"\"\n",
    "    \n",
    "    word = word.lower().replace(\" \", \"_\")  # Format for ConceptNet API\n",
    "    base_url = \"http://api.conceptnet.io\"\n",
    "\n",
    "    # ‚úÖ **Expanded synonym retrieval (directly interchangeable words)**\n",
    "    synonym_rels = [\"/r/IsA\", \"/r/Synonym\", \"/r/SimilarTo\"]\n",
    "    synonym_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in synonym_rels]\n",
    "\n",
    "    synonyms = set()\n",
    "    for url in synonym_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            synonyms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Expanded related term retrieval (broader conceptual connections)**\n",
    "    related_rels = [\"/r/PartOf\", \"/r/HasA\", \"/r/UsedFor\", \"/r/DerivedFrom\", \"/r/RelatedTo\"]\n",
    "    related_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in related_rels]\n",
    "\n",
    "    related_terms = set()\n",
    "    for url in related_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            related_terms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Weight-based filtering for related terms**\n",
    "    related_url = f\"{base_url}/related/c/en/{word}?filter=/c/en\"\n",
    "    related_response = requests.get(related_url).json()\n",
    "\n",
    "    weighted_related_terms = sorted(\n",
    "        [(edge[\"@id\"].split(\"/\")[-1].replace(\"_\", \" \"), edge[\"weight\"]) \n",
    "         for edge in related_response.get(\"related\", []) if edge[\"weight\"] > weight_threshold],\n",
    "        key=lambda x: x[1], reverse=True  # Sort by weight (highest first)\n",
    "    )\n",
    "\n",
    "    # Merge weighted related terms\n",
    "    for term, _ in weighted_related_terms[:top_n]:\n",
    "        related_terms.add(term)\n",
    "\n",
    "    # ‚úÖ **Filter out near-duplicates and self-referential terms**\n",
    "    def is_too_similar(word, seen_words):\n",
    "        \"\"\"Check if a word is too similar to an already included word using similarity ratio.\"\"\"\n",
    "        return any(difflib.SequenceMatcher(None, word, seen).ratio() > similarity_threshold for seen in seen_words)\n",
    "\n",
    "    def is_containing_original(word, original):\n",
    "        \"\"\"Check if the word contains the original answer or is an exact match.\"\"\"\n",
    "        return word.lower() == original.lower() or original.lower() in word.lower()\n",
    "\n",
    "    # Filter synonyms & related terms for uniqueness and no self-reference\n",
    "    filtered_synonyms = []\n",
    "    seen_words = set()\n",
    "    for syn in synonyms:\n",
    "        if not is_too_similar(syn, seen_words) and not is_containing_original(syn, word):\n",
    "            filtered_synonyms.append(syn)\n",
    "            seen_words.add(syn)\n",
    "\n",
    "    filtered_related = []\n",
    "    seen_words = set()\n",
    "    for rel in related_terms:\n",
    "        if not is_too_similar(rel, seen_words) and not is_containing_original(rel, word):\n",
    "            filtered_related.append(rel)\n",
    "            seen_words.add(rel)\n",
    "\n",
    "    return filtered_synonyms[:top_n], filtered_related[:top_n]\n",
    "\n",
    "# ‚úÖ **Test Cases**\n",
    "test_words = [\"Shakespeare\", \"Einstein\", \"Nike\", \"Black Sea\", \"Amazon\", \"Physics\"]\n",
    "\n",
    "print(\"\\nüîπ **Testing Expanded ConceptNet Synonyms & Related Terms**\")\n",
    "for word in test_words:\n",
    "    synonyms, related_terms = get_conceptnet_synonyms_and_related(word)\n",
    "    print(f\"üîπ **ConceptNet results for '{word}':**\")\n",
    "    print(f\"   - Synonyms: {synonyms}\")\n",
    "    print(f\"   - Related Terms: {related_terms}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a1f6a8-ae06-4592-a8ff-eead03e19df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing Combined ConceptNet & WordNet Synonyms & Related Terms**\n",
      "üîπ **ConceptNet & WordNet results for 'Shakespeare':**\n",
      "   - Synonyms: ['Bard of Avon', 'a great dramatist', 'William Shakspere']\n",
      "   - Related Terms: ['shakespearian', 'alfred lord tennyson', 'seventeenth', 'harold pinter', 'english', 'christopher marlowe', 'macbeth', 'poet', 'dramatist', 'centuries']\n",
      "\n",
      "üîπ **ConceptNet & WordNet results for 'Einstein':**\n",
      "   - Synonyms: ['a very intelligent man', 'a physicist', 'brain', 'mastermind', 'genius']\n",
      "   - Related Terms: ['mole', 'e mc', 'smart', 'relativity', 'paul dirac', 'theoretical physicist', 'frequency', 'stephen hawking', 'isaac newton', 'photon']\n",
      "\n",
      "üîπ **ConceptNet & WordNet results for 'Nike':**\n",
      "   - Synonyms: ['information appliance', 'sneaks']\n",
      "   - Related Terms: ['goddess', 'triumph', 'adidas', 'athena', 'victory', 'reebok', 'sneakers', 'asteroid', 'tennis shoes', 'jordans']\n",
      "\n",
      "üîπ **ConceptNet & WordNet results for 'Black Sea':**\n",
      "   - Synonyms: ['sea', 'Euxine Sea']\n",
      "   - Related Terms: ['euxinian', 'eastern europe', 'transcaucasus', 'yellow sea', 'white sea', 'tuapse', 'caucasia', 'novorossiysk', 'caspian sea', 'turkey']\n",
      "\n",
      "üîπ **ConceptNet & WordNet results for 'Amazon':**\n",
      "   - Synonyms: ['virago', 'parrot', 'mythical being', 'woman', 'sentient animal', 'video game', 'fictional female person', 'a river']\n",
      "   - Related Terms: ['atlantic', 'obliterate', 'brazil', 'parrot', 'black sea', 'Peru', 'igarap√©', 'xingu', 'south america', 'maloca']\n",
      "\n",
      "üîπ **ConceptNet & WordNet results for 'Physics':**\n",
      "   - Synonyms: ['aperient', 'purgative', 'cathartic', 'natural science', 'physic', 'natural philosophy']\n",
      "   - Related Terms: ['college', 'applied', 'energy', 'matter', 'quantum entanglement', 'scientific', 'field', 'interaction', 'has laws', 'physicist']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VERY GOOD, GOOD FILTERING\n",
    "#Combine with wordnet\n",
    "import requests\n",
    "import difflib\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_wordnet_synonyms(word):\n",
    "    \"\"\"Fetch synonyms from WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):  # Fixed reference to 'wn'\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_conceptnet_synonyms_and_related(word, top_n=10, weight_threshold=0.5, similarity_threshold=0.6):\n",
    "    \"\"\"Fetch synonyms and related words from ConceptNet and WordNet, ensuring diversity and relevance.\"\"\"\n",
    "    \n",
    "    word = word.lower().replace(\" \", \"_\")  # Format for ConceptNet API\n",
    "    base_url = \"http://api.conceptnet.io\"\n",
    "\n",
    "    # ‚úÖ **Expanded synonym retrieval**\n",
    "    synonym_rels = [\"/r/IsA\", \"/r/Synonym\", \"/r/SimilarTo\"]\n",
    "    synonym_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in synonym_rels]\n",
    "\n",
    "    synonyms = set()\n",
    "    for url in synonym_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            synonyms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Integrate WordNet synonyms**\n",
    "    wordnet_synonyms = get_wordnet_synonyms(word)\n",
    "    synonyms.update(wordnet_synonyms)\n",
    "\n",
    "    # ‚úÖ **Expanded related term retrieval**\n",
    "    related_rels = [\"/r/PartOf\", \"/r/HasA\", \"/r/UsedFor\", \"/r/DerivedFrom\", \"/r/RelatedTo\"]\n",
    "    related_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in related_rels]\n",
    "\n",
    "    related_terms = set()\n",
    "    for url in related_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            related_terms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Weight-based filtering for related terms**\n",
    "    related_url = f\"{base_url}/related/c/en/{word}?filter=/c/en\"\n",
    "    related_response = requests.get(related_url).json()\n",
    "\n",
    "    weighted_related_terms = sorted(\n",
    "        [(edge[\"@id\"].split(\"/\")[-1].replace(\"_\", \" \"), edge[\"weight\"]) \n",
    "         for edge in related_response.get(\"related\", []) if edge[\"weight\"] > weight_threshold],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    for term, _ in weighted_related_terms[:top_n]:\n",
    "        related_terms.add(term)\n",
    "\n",
    "    # ‚úÖ **Filter out near-duplicates and self-referential terms**\n",
    "    def is_too_similar(word, seen_words):\n",
    "        return any(difflib.SequenceMatcher(None, word, seen).ratio() > similarity_threshold for seen in seen_words)\n",
    "\n",
    "    def is_containing_original(word, original):\n",
    "        \"\"\"Check if the word is an exact match (ignoring case) or contains the original term.\"\"\"\n",
    "        word_lower = word.lower().replace(\"_\", \" \")\n",
    "        original_lower = original.lower().replace(\"_\", \" \")\n",
    "        \n",
    "        return word_lower == original_lower or original_lower in word_lower \n",
    "\n",
    "\n",
    "\n",
    "    # Filter synonyms & related terms for uniqueness and no self-reference\n",
    "    filtered_synonyms = []\n",
    "    seen_words = set()\n",
    "    for syn in synonyms:\n",
    "        if not is_too_similar(syn, seen_words) and not is_containing_original(syn, word):\n",
    "            filtered_synonyms.append(syn)\n",
    "            seen_words.add(syn)\n",
    "\n",
    "    filtered_related = []\n",
    "    seen_words = set()\n",
    "    for rel in related_terms:\n",
    "        if not is_too_similar(rel, seen_words) and not is_containing_original(rel, word):\n",
    "            filtered_related.append(rel)\n",
    "            seen_words.add(rel)\n",
    "\n",
    "    return filtered_synonyms[:top_n], filtered_related[:top_n]\n",
    "\n",
    "# ‚úÖ **Test Cases**\n",
    "test_words = [\"Shakespeare\", \"Einstein\", \"Nike\", \"Black Sea\", \"Amazon\", \"Physics\"]\n",
    "\n",
    "print(\"\\nüîπ **Testing Combined ConceptNet & WordNet Synonyms & Related Terms**\")\n",
    "for word in test_words:\n",
    "    synonyms, related_terms = get_conceptnet_synonyms_and_related(word)\n",
    "    print(f\"üîπ **ConceptNet & WordNet results for '{word}':**\")\n",
    "    print(f\"   - Synonyms: {synonyms}\")\n",
    "    print(f\"   - Related Terms: {related_terms}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1afa6e-e190-4718-8720-231de7422515",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Final Embedding Approach: Combine ConceptNet + Wordnet, use Word2Vec as fallback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596d8ecd-28b1-4aac-ab98-93b631633232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\n",
      "üîπ **Results for 'Shakespeare':**\n",
      "   - Synonyms: ['Bard of Avon', 'a great dramatist', 'William Shakspere', 'Shakspere', 'www.angelfire.com']\n",
      "   - Related Terms: ['shakespearian', 'alfred lord tennyson', 'seventeenth', 'harold pinter', 'english', 'sixteenth', 'christopher marlowe', 'macbeth', 'poet', 'dramatist']\n",
      "\n",
      "üîπ **Results for 'Einstein':**\n",
      "   - Synonyms: ['a very intelligent man', 'a physicist', 'brain', 'mastermind', 'brainiac']\n",
      "   - Related Terms: ['mole', 'e mc', 'smart', 'relativity', 'paul dirac', 'theoretical physicist', 'frequency', 'stephen hawking', 'isaac newton', 'photon']\n",
      "\n",
      "üîπ **Results for 'Nike':**\n",
      "   - Synonyms: ['information appliance', 'sneaks', 'schuhe']\n",
      "   - Related Terms: ['goddess', 'triumph', 'adidas', 'athena', 'victory', 'reebok', 'sneakers', 'asteroid', 'tennis shoes', 'jordans']\n",
      "\n",
      "üîπ **Results for 'Black Sea':**\n",
      "   - Synonyms: ['sea', 'Euxine Sea']\n",
      "   - Related Terms: ['euxinian', 'eastern europe', 'transcaucasus', 'yellow sea', 'white sea', 'tuapse', 'caucasia', 'novorossiysk', 'caspian sea', 'turkey']\n",
      "\n",
      "üîπ **Results for 'Amazon':**\n",
      "   - Synonyms: ['virago', 'parrot', 'mythical being', 'woman', 'sentient animal']\n",
      "   - Related Terms: ['atlantic', 'obliterate', 'brazil', 'parrot', 'black sea', 'warrior', 'Peru', 'igarap√©', 'xingu', 'south america']\n",
      "\n",
      "üîπ **Results for 'Physics':**\n",
      "   - Synonyms: ['aperient', 'purgative', 'cathartic', 'natural science', 'a science']\n",
      "   - Related Terms: ['college', 'applied', 'energy', 'matter', 'quantum entanglement', 'scientific', 'scientific field', 'field', 'interaction', 'has laws']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import difflib\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# ‚úÖ **Get WordNet Synonyms**\n",
    "def get_wordnet_synonyms(word):\n",
    "    \"\"\"Fetch synonyms from WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "# ‚úÖ **Get Word2Vec Similar Words (Only as Backup)**\n",
    "def get_word2vec_similar_words(word, top_n=5):\n",
    "    \"\"\"Fetch similar words from Word2Vec if the word exists in vocabulary.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in wv:\n",
    "        return [w[0] for w in wv.most_similar(word, topn=top_n)]\n",
    "    return []\n",
    "\n",
    "# ‚úÖ **Get ConceptNet Synonyms & Related Words**\n",
    "def get_conceptnet_synonyms_and_related(word, min_synonyms=5, min_related=10, weight_threshold=0.5, similarity_threshold=0.8):\n",
    "    \"\"\"Fetch synonyms and related words from ConceptNet and WordNet, ensuring diversity and relevance.\"\"\"\n",
    "    \n",
    "    word = word.lower().replace(\" \", \"_\")  # Format for ConceptNet API\n",
    "    base_url = \"http://api.conceptnet.io\"\n",
    "\n",
    "    # ‚úÖ **Expanded synonym retrieval**\n",
    "    synonym_rels = [\"/r/IsA\", \"/r/Synonym\", \"/r/SimilarTo\"]\n",
    "    synonym_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in synonym_rels]\n",
    "\n",
    "    synonyms = set()\n",
    "    for url in synonym_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            synonyms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Include WordNet synonyms**\n",
    "    wordnet_synonyms = get_wordnet_synonyms(word)\n",
    "    synonyms.update(wordnet_synonyms)\n",
    "\n",
    "    # ‚úÖ **Expanded related term retrieval**\n",
    "    related_rels = [\"/r/PartOf\", \"/r/HasA\", \"/r/UsedFor\", \"/r/DerivedFrom\", \"/r/RelatedTo\"]\n",
    "    related_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in related_rels]\n",
    "\n",
    "    related_terms = set()\n",
    "    for url in related_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            related_terms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Weight-based filtering for related terms**\n",
    "    related_url = f\"{base_url}/related/c/en/{word}?filter=/c/en\"\n",
    "    related_response = requests.get(related_url).json()\n",
    "\n",
    "    weighted_related_terms = sorted(\n",
    "        [(edge[\"@id\"].split(\"/\")[-1].replace(\"_\", \" \"), edge[\"weight\"]) \n",
    "         for edge in related_response.get(\"related\", []) if edge[\"weight\"] > weight_threshold],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    for term, _ in weighted_related_terms[:min_related]:\n",
    "        related_terms.add(term)\n",
    "\n",
    "    # ‚úÖ **Filter out near-duplicates and self-referential terms**\n",
    "    def is_too_similar(word, seen_words):\n",
    "        \"\"\"Check if a word is too similar to an already included word using similarity ratio.\"\"\"\n",
    "        return any(difflib.SequenceMatcher(None, word, seen).ratio() > similarity_threshold for seen in seen_words)\n",
    "\n",
    "    def is_containing_original(word, original):\n",
    "        \"\"\"Check if the word is an exact match (ignoring case) or contains the original term.\"\"\"\n",
    "        word_lower = word.lower().replace(\"_\", \" \")\n",
    "        original_lower = original.lower().replace(\"_\", \" \")\n",
    "        return word_lower == original_lower or original_lower in word_lower\n",
    "\n",
    "    # ‚úÖ Filter synonyms & related terms for uniqueness and no self-reference\n",
    "    filtered_synonyms = []\n",
    "    seen_words = set()\n",
    "    for syn in synonyms:\n",
    "        if not is_too_similar(syn, seen_words) and not is_containing_original(syn, word):\n",
    "            filtered_synonyms.append(syn)\n",
    "            seen_words.add(syn)\n",
    "\n",
    "    filtered_related = []\n",
    "    seen_words = set()\n",
    "    for rel in related_terms:\n",
    "        if not is_too_similar(rel, seen_words) and not is_containing_original(rel, word):\n",
    "            filtered_related.append(rel)\n",
    "            seen_words.add(rel)\n",
    "\n",
    "    # ‚úÖ **Use Word2Vec Backup if Needed**\n",
    "    if len(filtered_synonyms) < min_synonyms:\n",
    "        word2vec_synonyms = get_word2vec_similar_words(word, top_n=min_synonyms - len(filtered_synonyms))\n",
    "        for w2v in word2vec_synonyms:\n",
    "            if not is_too_similar(w2v, seen_words) and not is_containing_original(w2v, word):\n",
    "                filtered_synonyms.append(w2v)\n",
    "                seen_words.add(w2v)\n",
    "\n",
    "    if len(filtered_related) < min_related:\n",
    "        word2vec_related = get_word2vec_similar_words(word, top_n=min_related - len(filtered_related))\n",
    "        for w2v in word2vec_related:\n",
    "            if not is_too_similar(w2v, seen_words) and not is_containing_original(w2v, word):\n",
    "                filtered_related.append(w2v)\n",
    "                seen_words.add(w2v)\n",
    "\n",
    "    return filtered_synonyms[:min_synonyms], filtered_related[:min_related]\n",
    "\n",
    "# ‚úÖ **Test Cases**\n",
    "test_words = [\"Shakespeare\", \"Einstein\", \"Nike\", \"Black Sea\", \"Amazon\", \"Physics\"]\n",
    "\n",
    "print(\"\\nüîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\")\n",
    "for word in test_words:\n",
    "    synonyms, related_terms = get_conceptnet_synonyms_and_related(word)\n",
    "    print(f\"üîπ **Results for '{word}':**\")\n",
    "    print(f\"   - Synonyms: {synonyms}\")\n",
    "    print(f\"   - Related Terms: {related_terms}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd514cac-b0dc-4679-8ad4-76a9506cb66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\n",
      "üîπ **Results for 'Shakespeare':**\n",
      "   - Synonyms: ['William Shakspere', 'Bard of Avon', 'home.htm']\n",
      "   - Related Terms: ['shakespearian', 'sixteenth', 'christopher marlowe', 'harold pinter', 'playwright', 'home.htm']\n",
      "\n",
      "üîπ **Results for 'Einstein':**\n",
      "   - Synonyms: ['a physicist']\n",
      "   - Related Terms: ['theoretical physicist', 'relativity', 'theory of relativity', 'photon', 'armstrong']\n",
      "\n",
      "üîπ **Results for 'Nike':**\n",
      "   - Synonyms: ['information appliance']\n",
      "   - Related Terms: ['victory', 'tennis shoes', 'athletic footwear', 'athena', 'christian louboutin']\n",
      "\n",
      "üîπ **Results for 'Black Sea':**\n",
      "   - Synonyms: ['black sea', 'Black Sea']\n",
      "   - Related Terms: ['yellow sea', 'white sea', 'inland sea', 'southeastern europe', 'red sea']\n",
      "\n",
      "üîπ **Results for 'Amazon':**\n",
      "   - Synonyms: ['fictional female person', 'mythical being']\n",
      "   - Related Terms: ['overwhelm', 'warrior', 'south america']\n",
      "\n",
      "üîπ **Results for 'Physics':**\n",
      "   - Synonyms: ['cathartic', 'physical science', 'natural philosophy']\n",
      "   - Related Terms: ['math', 'quantum theory', 'chemistry', 'applied mathematics', 'physical', 'interaction', 'physicist', 'mathematics']\n",
      "\n",
      "üîπ **Results for 'Isaac Newton':**\n",
      "   - Synonyms: ['Newton', 'an ambitious man', 'Sir Isaac Newton']\n",
      "   - Related Terms: ['theoretical physicist', 'newtons']\n",
      "\n",
      "üîπ **Results for 'Neural Networks':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['computational algorithms']\n",
      "\n",
      "üîπ **Results for 'Cryptography':**\n",
      "   - Synonyms: ['secret writing', 'writing', 'cryptanalytics', 'cryptographic']\n",
      "   - Related Terms: ['repudiation', 'cryptographically', 'cryptographic', 'cryptosystem', 'authentication', 'security', 'cryptographer', 'crypt', 'cryptanalytic', 'cryptographic']\n",
      "\n",
      "üîπ **Results for 'Pi':**\n",
      "   - Synonyms: ['sherlock', '...rounded to 3.14', 'an transcedental number', 'approximately 3.1415926', 'an irrational number']\n",
      "   - Related Terms: ['ratio', 'the value of about 3.141592653', 'classical greek', 'phis']\n",
      "\n",
      "üîπ **Results for 'Tesla':**\n",
      "   - Synonyms: ['an inventor', 'FT ICR']\n",
      "   - Related Terms: ['international system of units', 'inductivity', 'FT ICR']\n",
      "\n",
      "üîπ **Results for 'Nintendo':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['playstation']\n",
      "\n",
      "üîπ **Results for 'McDonald's':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['fast food']\n",
      "\n",
      "üîπ **Results for 'Chair':**\n",
      "   - Synonyms: ['hot seat', 'non powered device', 'professorship', 'seat', 'hexalateral object']\n",
      "   - Related Terms: ['reach high cupboards', 'four', 'victory', 'electrocuting the condemend', 'distinguished', 'sitting down', 'stand on', 'people', 'offer hospitality', 'resting']\n",
      "\n",
      "üîπ **Results for 'Backpack':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['mount', 'backback']\n",
      "\n",
      "üîπ **Results for 'Airplane':**\n",
      "   - Synonyms: ['aeroplane', 'heavier-than-air craft', 'air transportation vehicle', 'vehicle', 'a large machine with wings']\n",
      "   - Related Terms: ['flying machine', 'aeroplane', 'traversing the skies', 'going from oneplace to another', 'crashing into buildings', 'heavier than air', 'wing']\n",
      "\n",
      "üîπ **Results for 'Relativity':**\n",
      "   - Synonyms: ['theory', 'scientific theory']\n",
      "   - Related Terms: ['relative', 'quantum theory', 'relativistically', 'physics', 'quantum theory', 'Einstein theory']\n",
      "\n",
      "üîπ **Results for 'Artificial Intelligence':**\n",
      "   - Synonyms: ['USA Subjex Corporation']\n",
      "   - Related Terms: ['think', 'self aware', 'thought', 'turing test', 'understanding the human mind and brains.', 'speech recognition', 'chatbot', 'USA Subjex Corporation']\n",
      "\n",
      "üîπ **Results for 'Elon Musk':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['lengthened', 'elongated', 'stretch']\n",
      "\n",
      "üîπ **Results for 'Nikola Tesla':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This time, loosen thresholds if not enough words found. More filtering \n",
    "import requests\n",
    "import difflib\n",
    "import langdetect\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def get_wordnet_synonyms(word):\n",
    "    \"\"\"Fetch synonyms from WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_word2vec_synonyms(word, top_n=5):\n",
    "    \"\"\"Fetch similar words from Word2Vec if the word exists in vocabulary.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in wv:\n",
    "        return [w[0] for w in wv.most_similar(word, topn=top_n)]\n",
    "    return []\n",
    "\n",
    "def filter_results(words, original_word):\n",
    "    \"\"\"Remove non-English words, URLs, underscores, and overly similar words.\"\"\"\n",
    "    filtered = set()\n",
    "    for word in words:\n",
    "        word_clean = word.replace('_', ' ')\n",
    "        try:\n",
    "            if \"www\" in word or \".com\" in word or \".net\" in word:\n",
    "                continue  # Remove URLs\n",
    "            if langdetect.detect(word_clean) != \"en\":\n",
    "                continue  # Remove non-English words\n",
    "            if word_clean.lower() == original_word.lower() or original_word.lower() in word_clean.lower():\n",
    "                continue  # Avoid self-referential words\n",
    "            filtered.add(word_clean)\n",
    "        except:\n",
    "            continue\n",
    "    return list(filtered)\n",
    "\n",
    "def get_conceptnet_synonyms_and_related(word, min_synonyms=5, min_related=10, weight_threshold=0.5, similarity_threshold=0.8):\n",
    "    \"\"\"Fetch synonyms and related words from ConceptNet, WordNet, and Word2Vec.\"\"\"\n",
    "    word = word.lower().replace(\" \", \"_\")\n",
    "    base_url = \"http://api.conceptnet.io\"\n",
    "    \n",
    "    # ‚úÖ **Fetch synonyms**\n",
    "    synonym_rels = [\"/r/IsA\", \"/r/Synonym\", \"/r/SimilarTo\"]\n",
    "    synonym_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in synonym_rels]\n",
    "    synonyms = set()\n",
    "    for url in synonym_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            synonyms.add(edge['end']['label'])\n",
    "    synonyms.update(get_wordnet_synonyms(word))\n",
    "    \n",
    "    # ‚úÖ **Fetch related terms**\n",
    "    related_rels = [\"/r/PartOf\", \"/r/HasA\", \"/r/UsedFor\", \"/r/DerivedFrom\", \"/r/RelatedTo\"]\n",
    "    related_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in related_rels]\n",
    "    related_terms = set()\n",
    "    for url in related_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            related_terms.add(edge['end']['label'])\n",
    "    \n",
    "    # ‚úÖ **Weight-based filtering for related terms**\n",
    "    related_url = f\"{base_url}/related/c/en/{word}?filter=/c/en\"\n",
    "    related_response = requests.get(related_url).json()\n",
    "    weighted_related_terms = sorted(\n",
    "        [(edge[\"@id\"].split(\"/\")[-1].replace(\"_\", \" \"), edge[\"weight\"]) \n",
    "         for edge in related_response.get(\"related\", []) if edge[\"weight\"] > weight_threshold],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    for term, _ in weighted_related_terms[:min_related]:\n",
    "        related_terms.add(term)\n",
    "    \n",
    "    # ‚úÖ **Apply filtering**\n",
    "    filtered_synonyms = filter_results(synonyms, word)\n",
    "    filtered_related = filter_results(related_terms, word)\n",
    "    \n",
    "    # ‚úÖ **Ensure minimum outputs, fallback to Word2Vec if needed**\n",
    "    if len(filtered_synonyms) < min_synonyms:\n",
    "        word2vec_synonyms = get_word2vec_synonyms(word, top_n=min_synonyms - len(filtered_synonyms))\n",
    "        filtered_synonyms.extend(filter_results(word2vec_synonyms, word))\n",
    "    \n",
    "    if len(filtered_related) < min_related:\n",
    "        word2vec_related = get_word2vec_synonyms(word, top_n=min_related - len(filtered_related))\n",
    "        filtered_related.extend(filter_results(word2vec_related, word))\n",
    "    \n",
    "    return filtered_synonyms[:min_synonyms], filtered_related[:min_related]\n",
    "\n",
    "# ‚úÖ **Test Cases**\n",
    "test_words = [\n",
    "    \"Shakespeare\", \"Einstein\", \"Nike\", \"Black Sea\", \"Amazon\", \"Physics\",\n",
    "    \"Isaac Newton\", \"Neural Networks\", \"Cryptography\", \"Pi\",\n",
    "    \"Tesla\", \"Nintendo\", \"McDonald's\", \"Chair\", \"Backpack\", \"Airplane\",\n",
    "    \"Relativity\", \"Artificial Intelligence\", \"Elon Musk\", \"Nikola Tesla\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\")\n",
    "for word in test_words:\n",
    "    synonyms, related_terms = get_conceptnet_synonyms_and_related(word)\n",
    "    print(f\"üîπ **Results for '{word}':**\")\n",
    "    print(f\"   - Synonyms: {synonyms}\")\n",
    "    print(f\"   - Related Terms: {related_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b699bda-9e46-4a75-8318-c3640c9d0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\n",
      "üîπ **Results for 'Shakespeare':**\n",
      "   - Synonyms: ['Bard of Avon', 'William Shakspere']\n",
      "   - Related Terms: ['shakespearian', 'harold pinter', 'sixteenth', 'christopher marlowe', 'playwright']\n",
      "\n",
      "üîπ **Results for 'Einstein':**\n",
      "   - Synonyms: ['a physicist', 'armstrong']\n",
      "   - Related Terms: ['relativity', 'theoretical physicist', 'photon']\n",
      "\n",
      "üîπ **Results for 'Nike':**\n",
      "   - Synonyms: ['information appliance']\n",
      "   - Related Terms: ['athena', 'victory', 'athletic footwear']\n",
      "\n",
      "üîπ **Results for 'Black Sea':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['yellow sea', 'white sea', 'inland sea', 'southeastern europe']\n",
      "\n",
      "üîπ **Results for 'Amazon':**\n",
      "   - Synonyms: ['mythical being', 'fictional female person']\n",
      "   - Related Terms: ['warrior', 'south america', 'overwhelm']\n",
      "\n",
      "üîπ **Results for 'Physics':**\n",
      "   - Synonyms: ['cathartic', 'physical science']\n",
      "   - Related Terms: ['field', 'interaction', 'physicist', 'mathematics', 'math', 'quantum theory', 'chemistry']\n",
      "\n",
      "üîπ **Results for 'Neural Networks':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: []\n",
      "\n",
      "üîπ **Results for 'Cryptography':**\n",
      "   - Synonyms: ['secret writing', 'cryptanalytics', 'encryption_algorithms']\n",
      "   - Related Terms: ['cryptographically', 'crypt', 'repudiation', 'authentication', 'security', 'cryptanalytic', 'cryptosystem']\n",
      "\n",
      "üîπ **Results for 'Pi':**\n",
      "   - Synonyms: ['...rounded to 3.14', 'protease inhibitor', 'a mathematical concept', 'approximately 3.141592653589793238462643383279502', 'an transcedental number']\n",
      "   - Related Terms: ['phis', 'the value of about 3.141592653', 'ratio', 'classical greek']\n",
      "\n",
      "üîπ **Results for 'Tesla':**\n",
      "   - Synonyms: ['an inventor', 'FT_ICR']\n",
      "   - Related Terms: ['international system of units', 'inductivity']\n",
      "\n",
      "üîπ **Results for 'Nintendo':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['playstation']\n",
      "\n",
      "üîπ **Results for 'McDonald's':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['fast food']\n",
      "\n",
      "üîπ **Results for 'Backpack':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['mount']\n",
      "\n",
      "üîπ **Results for 'Airplane':**\n",
      "   - Synonyms: ['aeroplane', 'a large machine with wings', 'heavier-than-air craft', 'vehicle', 'heavier than the air it displaces']\n",
      "   - Related Terms: ['transport people', 'visit England', 'heavier than air', 'crashing into buildings', 'aeroplane', 'flying machine', 'traversing the skies', 'flight', 'going from oneplace to another']\n",
      "\n",
      "üîπ **Results for 'Relativity':**\n",
      "   - Synonyms: ['theory', 'scientific theory']\n",
      "   - Related Terms: ['relative', 'quantum theory', 'physics', 'relativistically']\n",
      "\n",
      "üîπ **Results for 'Artificial Intelligence':**\n",
      "   - Synonyms: ['USA_Subjex_Corporation']\n",
      "   - Related Terms: ['think', 'thought', 'understanding the human mind and brains.', 'turing test', 'speech recognition', 'chatbot']\n",
      "\n",
      "üîπ **Results for 'Elon Musk':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['lengthened', 'stretch']\n",
      "\n",
      "üîπ **Results for 'Nikola Tesla':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: []\n",
      "\n",
      "üîπ **Results for 'Olympics':**\n",
      "   - Synonyms: ['an international sporting event', 'Olympic Games', 'held every four years', 'a large set of competitive events']\n",
      "   - Related Terms: ['paralympic games', 'olympic']\n",
      "\n",
      "üîπ **Results for 'Wimbledon':**\n",
      "   - Synonyms: ['a city in England']\n",
      "   - Related Terms: ['wembley', 'tournament', 'borough']\n",
      "\n",
      "üîπ **Results for 'Adidas':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['activewear', 'sportswear']\n",
      "\n",
      "üîπ **Results for 'Leonardo da Vinci':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: []\n",
      "\n",
      "üîπ **Results for 'Cleopatra':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['hatshepsut']\n",
      "\n",
      "üîπ **Results for 'Hercules':**\n",
      "   - Synonyms: ['comics character', 'Heracles', 'wrestler']\n",
      "   - Related Terms: ['city', 'hera', 'northern', 'california']\n",
      "\n",
      "üîπ **Results for 'Quantum Computing':**\n",
      "   - Synonyms: ['Spintronics']\n",
      "   - Related Terms: ['computing', 'time complexity', 'phenomenon']\n",
      "\n",
      "üîπ **Results for 'CRISPR':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: []\n",
      "\n",
      "üîπ **Results for 'Lord of the Rings':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['privy councillor']\n",
      "\n",
      "üîπ **Results for 'Marvel':**\n",
      "   - Synonyms: []\n",
      "   - Related Terms: ['comic book']\n",
      "\n",
      "üîπ **Results for 'Cyberpunk':**\n",
      "   - Synonyms: ['cyber-terrorist', 'hard Sci-fi', 'writer', 'science_fiction']\n",
      "   - Related Terms: ['information technology', 'post apocalyptic', 'afrofuturism', 'synthesizer']\n",
      "\n",
      "üîπ **Results for 'Bicycle':**\n",
      "   - Synonyms: ['a vehicle with two wheels', 'wheel', 'much smaller than a fire truck', 'wheeled vehicle', 'toy']\n",
      "   - Related Terms: ['ride to town', 'travel short distances', 'two wheels and pedals', 'a chain that drives the wheels', 'vehicle', 'get faster to the mall', 'motorcycle', 'bycicle', 'two tires', 'Racing']\n",
      "\n",
      "üîπ **Results for 'Cooking':**\n",
      "   - Synonyms: ['happening now, cooked', 'what a cook does', 'a way to prepare food', 'cookery', 'ready']\n",
      "   - Related Terms: ['sharing food with friends', 'changing the consistency of food', 'making food easier to eat', 'cookery', 'fixing meals', 'people who know how', 'relaxation', 'preventing food poisioning', 'the feeding of the family', 'preparing raw food for consumption']\n",
      "\n",
      "üîπ **Results for 'Programming':**\n",
      "   - Synonyms: ['an activity', 'creating by mental acts', 'computer programing']\n",
      "   - Related Terms: ['instructions', 'toplevel', 'brain washing', 'operating']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final version\n",
    "import requests\n",
    "import difflib\n",
    "import langdetect\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "def get_wordnet_synonyms(word):\n",
    "    \"\"\"Fetch synonyms from WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def get_word2vec_similar_words(word, top_n=5):\n",
    "    \"\"\"Fetch similar words from Word2Vec if the word exists in vocabulary.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in wv:\n",
    "        return [w[0] for w in wv.most_similar(word, topn=top_n)]\n",
    "    return []\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"Filter out non-English words, URLs, and junk values.\"\"\"\n",
    "    if any(substr in word for substr in [\"www.\", \".com\", \".net\", \"home.htm\"]):\n",
    "        return False\n",
    "    try:\n",
    "        if langdetect.detect(word) != \"en\":\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_conceptnet_synonyms_and_related(word, min_synonyms=5, min_related=10, weight_threshold=0.5, similarity_threshold=0.6):\n",
    "    \"\"\"Fetch synonyms and related words from ConceptNet, WordNet, and Word2Vec.\"\"\"\n",
    "    word = word.lower().replace(\" \", \"_\")  # Format for ConceptNet API\n",
    "    base_url = \"http://api.conceptnet.io\"\n",
    "\n",
    "    # ‚úÖ **Expanded synonym retrieval**\n",
    "    synonym_rels = [\"/r/IsA\", \"/r/Synonym\", \"/r/SimilarTo\"]\n",
    "    synonym_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in synonym_rels]\n",
    "\n",
    "    synonyms = set()\n",
    "    for url in synonym_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            synonyms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Integrate WordNet synonyms**\n",
    "    wordnet_synonyms = get_wordnet_synonyms(word)\n",
    "    synonyms.update(wordnet_synonyms)\n",
    "    \n",
    "    # ‚úÖ **Expanded related term retrieval**\n",
    "    related_rels = [\"/r/PartOf\", \"/r/HasA\", \"/r/UsedFor\", \"/r/DerivedFrom\", \"/r/RelatedTo\"]\n",
    "    related_urls = [f\"{base_url}/query?rel={rel}&start=/c/en/{word}&end=/c/en\" for rel in related_rels]\n",
    "\n",
    "    related_terms = set()\n",
    "    for url in related_urls:\n",
    "        response = requests.get(url).json()\n",
    "        for edge in response.get('edges', []):\n",
    "            related_terms.add(edge['end']['label'])\n",
    "\n",
    "    # ‚úÖ **Weight-based filtering for related terms**\n",
    "    related_url = f\"{base_url}/related/c/en/{word}?filter=/c/en\"\n",
    "    related_response = requests.get(related_url).json()\n",
    "\n",
    "    weighted_related_terms = sorted(\n",
    "        [(edge[\"@id\"].split(\"/\")[-1].replace(\"_\", \" \"), edge[\"weight\"]) \n",
    "         for edge in related_response.get(\"related\", []) if edge[\"weight\"] > weight_threshold],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    for term, _ in weighted_related_terms[:min_related]:\n",
    "        related_terms.add(term)\n",
    "\n",
    "    # ‚úÖ **Filter out near-duplicates, non-English words, and junk**\n",
    "    def is_too_similar(word, seen_words):\n",
    "        return any(difflib.SequenceMatcher(None, word, seen).ratio() > similarity_threshold for seen in seen_words)\n",
    "\n",
    "    def is_containing_original(word, original):\n",
    "        word_lower = word.lower().replace(\"_\", \" \")\n",
    "        original_lower = original.lower().replace(\"_\", \" \")\n",
    "        return word_lower == original_lower or original_lower in word_lower\n",
    "\n",
    "    # Filter synonyms & related terms for uniqueness and no self-reference\n",
    "    filtered_synonyms = []\n",
    "    seen_words = set()\n",
    "    for syn in synonyms:\n",
    "        if not is_too_similar(syn, seen_words) and not is_containing_original(syn, word) and is_valid_word(syn):\n",
    "            filtered_synonyms.append(syn)\n",
    "            seen_words.add(syn)\n",
    "\n",
    "    filtered_related = []\n",
    "    seen_words = set()\n",
    "    for rel in related_terms:\n",
    "        if not is_too_similar(rel, seen_words) and not is_containing_original(rel, word) and is_valid_word(rel):\n",
    "            filtered_related.append(rel)\n",
    "            seen_words.add(rel)\n",
    "\n",
    "    # ‚úÖ **Ensure Minimum Synonyms & Related Terms**\n",
    "    if len(filtered_synonyms) < min_synonyms:\n",
    "        word2vec_synonyms = get_word2vec_similar_words(word, top_n=min_synonyms - len(filtered_synonyms))\n",
    "        for w2v in word2vec_synonyms:\n",
    "            if not is_too_similar(w2v, seen_words) and is_valid_word(w2v):\n",
    "                filtered_synonyms.append(w2v)\n",
    "                seen_words.add(w2v)\n",
    "\n",
    "    return filtered_synonyms[:min_synonyms], filtered_related[:min_related]\n",
    "\n",
    "# ‚úÖ **Test Cases**\n",
    "test_words = [\n",
    "    \"Shakespeare\", \"Einstein\", \"Nike\", \"Black Sea\", \"Amazon\", \"Physics\", \"Neural Networks\",\n",
    "    \"Cryptography\", \"Pi\", \"Tesla\", \"Nintendo\", \"McDonald's\", \"Backpack\", \"Airplane\", \"Relativity\",\n",
    "    \"Artificial Intelligence\", \"Elon Musk\", \"Nikola Tesla\", \"Olympics\", \"Wimbledon\", \"Adidas\",\n",
    "    \"Leonardo da Vinci\", \"Cleopatra\", \"Hercules\", \"Quantum Computing\", \"CRISPR\", \"Lord of the Rings\",\n",
    "    \"Marvel\", \"Cyberpunk\", \"Bicycle\", \"Cooking\", \"Programming\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîπ **Testing Final ConceptNet + WordNet + Word2Vec Backup**\")\n",
    "for word in test_words:\n",
    "    synonyms, related_terms = get_conceptnet_synonyms_and_related(word)\n",
    "    print(f\"üîπ **Results for '{word}':**\")\n",
    "    print(f\"   - Synonyms: {synonyms}\")\n",
    "    print(f\"   - Related Terms: {related_terms}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9913c4-918c-4f58-ba60-d80c420f0fc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hint Help 2: Classification of Clue Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58d420-a3ff-46ac-82ce-1cd747446aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification of Clue Types \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create labels for clue types (e.g., 0 = definition, 1 = anagram)\n",
    "df['ClueType'] = ...  # Add this column based on manual labeling\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Clue'], df['ClueType'], test_size=0.2)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict clue types\n",
    "y_pred = classifier.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5176f-8828-47ef-ba21-dc59dbb4a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall word vectors pointing in same direction are most similar\n",
    "#wv.most_similar('____')\n",
    "\n",
    "#Helper function to identify similarity between words, 1 = synonym, -1 = antonym, 0 = none\n",
    "def find_cosine(vec1, vec2):\n",
    "  # Scale vectors to both have unit length\n",
    "  unit_vec1 = vec1/np.linalg.norm(vec1)\n",
    "  unit_vec2 = vec2/np.linalg.norm(vec2)\n",
    "  # The dot product of unit vectors gives the cosine of their angle\n",
    "  return np.dot(unit_vec1,unit_vec2)\n",
    "\n",
    "#Getting sentence level vectors\n",
    "    #Naive approach - avg meaning vector \n",
    "    #more advanced - neural network with embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0f833-a050-4bb7-ae8a-51601e7de013",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hint Help 3: Fine tune transformer (BERT) to give hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80347d1-ed23-49bc-a944-33c697476712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_csv('deep_learning_nytcrosswords2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577b4cf-101c-4ad6-9df4-81e002d04654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21490a49-069e-4d7e-a42b-fb8a76116f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Should return False (CUDA is for NVIDIA)\n",
    "print(torch.backends.mps.is_available())  # Check if Metal is available (Mac users)\n",
    "print(torch.cuda.device_count())  # Should show 1+ if using ROCm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b503cd3-22eb-4a0b-aaab-48990636c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"BERT is ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700750a-7914-458b-bacd-42588aeb1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique answers dynamically\n",
    "num_unique_answers = df[\"Word\"].nunique()\n",
    "print(f\"Number of unique answers: {num_unique_answers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308dd6c-f0c4-46f9-b7a5-7c59c7ad311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#using uncased model for speed and performacne \n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_unique_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d094f3-dfde-4853-a5f8-48d11b8aaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the clues\n",
    "tokens = tokenizer(df[\"Clue\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert answers to numerical labels (assuming we have 5000 unique answers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df[\"Word\"])  # Converts text answers to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb919ca-afdf-45be-aa2c-286de3bb2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CrosswordDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokens.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = CrosswordDataset(tokens, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c265d7-af87-43ad-9790-286eb9d386db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if it worked so far:\n",
    "# Get the first item from the dataset\n",
    "first_sample = dataset[0]  # This should return a dictionary\n",
    "\n",
    "# Print the keys in the sample\n",
    "print(first_sample.keys())\n",
    "\n",
    "# Print the actual contents of the sample\n",
    "print(\"Input IDs:\", first_sample[\"input_ids\"])\n",
    "print(\"Attention Mask:\", first_sample[\"attention_mask\"])\n",
    "print(\"Label:\", first_sample[\"labels\"])\n",
    "\n",
    "print(\"Decoded Clue:\", tokenizer.decode(first_sample[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10f964-2a82-4602-9a8c-8ece4ad05c66",
   "metadata": {},
   "source": [
    "# üìå Fine-Tuning BERT for Crossword Solving\n",
    "\n",
    "## **1Ô∏è‚É£ Conceptual Overview**\n",
    "Fine-tuning BERT means **adapting a pre-trained language model** to specialize in **solving crossword clues**. Instead of training BERT from scratch, we **modify its last layers** so that it learns to map **crossword clues to correct answers**.\n",
    "\n",
    "üîπ **What we‚Äôre doing:**  \n",
    "- Giving BERT **crossword clues** as input.  \n",
    "- Training it to **predict the correct answer** (classification task).  \n",
    "- Using **supervised learning** (training with labeled crossword data).  \n",
    "- Adjusting BERT‚Äôs weights so it learns **patterns in crossword clues** over multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Technical Breakdown**\n",
    "### **1Ô∏è‚É£ Loading Pre-trained BERT Model**\n",
    "- We use `bert-base-uncased`, a pre-trained **Transformer model** that already understands English.  \n",
    "- Modify BERT‚Äôs **final layer** to classify one of many possible crossword answers.\n",
    "\n",
    "### **2Ô∏è‚É£ Tokenizing Data**\n",
    "- Convert crossword clues into **tokenized input** that BERT can understand.  \n",
    "- Convert answers into **numerical labels** using `LabelEncoder()`.\n",
    "\n",
    "### **3Ô∏è‚É£ Training Process (Fine-Tuning)**\n",
    "The fine-tuning process consists of:\n",
    "1. **Forward Pass:** BERT takes a **tokenized crossword clue** and predicts an answer.  \n",
    "2. **Loss Calculation:** Compare BERT‚Äôs predicted answer to the correct answer using **CrossEntropyLoss**.  \n",
    "3. **Backpropagation:** Compute gradients to understand **how much each weight contributed to the error**.  \n",
    "4. **Optimizer Update:** Adjust BERT‚Äôs weights using **Adam optimizer- Common optimization algo in DL. * to improve predictions.  \n",
    "5. **Repeat for Multiple Epochs:** The model gradually gets better at predicting correct answers.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Key Code Components**\n",
    "```python\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)  # Adjust BERT‚Äôs weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Measure how far off the predictions are\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(**inputs)  # Forward pass: Predict crossword answer\n",
    "        loss = loss_fn(outputs.logits, labels)  # Calculate loss\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71ffda-3c5b-4251-8bdf-3ece9321eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to GPU if available\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3307d3-140f-449b-b17d-a293a8a53475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778f8ec-be57-4075-8c8e-e61e85d3e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4b8d3-3961-480f-9f26-44ac7a30ff03",
   "metadata": {},
   "source": [
    "### Crossword Inputs Section\n",
    "- Option 1: manually type in hint and the answer --> not a very good option since you have to see the answer but simple enough\n",
    "- Option 2: manually type in just the clue, spaces used --> more realistic scenario but helper has to come up with the answer.\n",
    "- Option 3: use computer vision to scan the crossword clues and the crossword answers.\n",
    "    - Easiest/best/fastest solution, but requires user to have an answer key and also to look at it.\n",
    "- Option 4: use computer vision to scan empty crossword with hints. Helper has to come up with the answers on its own.\n",
    "    - Probably the most practical for a normal person doing a newspaper crossword without access to answers.\n",
    "    - So this is the ideal end goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a471a4-1111-4b40-931e-21e4cef9f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
